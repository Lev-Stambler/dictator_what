{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The ONNX File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gpt_neox.layers.0.mlp', 'gpt_neox.layers.0.mlp.dense_h_to_4h')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from special_neurons import get_most_negative_sets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "model_name = 'EleutherAI/pythia-160m'\n",
    "model_name = 'EleutherAI/pythia-70m'\n",
    "# model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model_og = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "most_neg = get_most_negative_sets(model_og)\n",
    "most_neg[0].prev_layer_name, most_neg[0].linear_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 2.0 False\n"
     ]
    }
   ],
   "source": [
    "# Our max_norm is none\n",
    "print(model_og.gpt_neox.embed_in.max_norm, model_og.gpt_neox.embed_in.norm_type, model_og.gpt_neox.embed_in.scale_grad_by_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel_og\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           GPTNeoXForCausalLM\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "GPTNeoXForCausalLM(\n",
      "           (gpt_neox): GPTNeoXModel(\n",
      "           (embed_in): Embedding(50304, 512)\n",
      "           (emb_dr <...> entwise_affine=True)\n",
      "           )\n",
      "           (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      "           )\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "GPTNeoX Model with a `language modeling` head on top for CLM fine-tuning.\n",
      "This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n",
      "it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
      "behavior.\n",
      "\n",
      "Parameters:\n",
      "    config ([`~GPTNeoXConfig`]): Model configuration class with all the parameters of the model.\n",
      "        Initializing with a config file does not load the weights associated with the model, only the\n",
      "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n"
     ]
    }
   ],
   "source": [
    "model_og?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((512,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].input_layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_matrix = model_og.gpt_neox.embed_in.weight\n",
    "embd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[25521,  1533]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_input_after_tokenizer(inp: str):\n",
    "\t\ttokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\t\tinp_out = tokenizer(inp, return_tensors=\"pt\")\n",
    "\t\tprint(inp_out)\n",
    "\t\tinputs = inp_out['input_ids'].squeeze(0)\n",
    "\t\t# print(inputs, inputs.shape)\n",
    "\t\t# TODO: IDK ABOUT WHATS GOING ON W/ TOKEN SIZE VS Vocab Size\n",
    "\t\t# Vocab size is 50204 and inp size is 50304\n",
    "\t\tone_hot = torch.zeros((inputs.shape[0], 50304), dtype=torch.int)\n",
    "\t\tfor i in range(inputs.shape[0]):\n",
    "\t\t\tone_hot[i, inputs[i]] = 1\n",
    "\t\t# one_hot[inputs['input_ids'][0, 0]] = 1\n",
    "\t\treturn one_hot#, inp_out['attention_mask']\n",
    "get_input_after_tokenizer('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[25521,  1533]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 50304]),\n",
       " tensor([[    0, 25521],\n",
       "         [    1,  1533]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps_one_hot = get_input_after_tokenizer(\"hello world\")\n",
    "inps_one_hot.shape, inps_one_hot.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelSel(\n",
       "  (embed_linear): Linear(in_features=50304, out_features=512, bias=False)\n",
       "  (layer_norm): SimplfiedLayerNorm(\n",
       "    (eps): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (ones_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (ones_linear_neg): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (attn): FixedAttentionMask(\n",
       "    (attn): GPTNeoXAttention(\n",
       "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (slice_query): Slicer(\n",
       "      (slicer): Linear(in_features=192, out_features=64, bias=True)\n",
       "    )\n",
       "    (slice_value): Slicer(\n",
       "      (slicer): Linear(in_features=192, out_features=64, bias=True)\n",
       "    )\n",
       "    (slice_key): Slicer(\n",
       "      (slicer): Linear(in_features=192, out_features=64, bias=True)\n",
       "    )\n",
       "    (slice_rotary): Slicer(\n",
       "      (slicer): Linear(in_features=64, out_features=16, bias=True)\n",
       "    )\n",
       "    (slice_non_rotary): Slicer(\n",
       "      (slicer): Linear(in_features=64, out_features=48, bias=True)\n",
       "    )\n",
       "    (slice_rorate_half_1): Slicer(\n",
       "      (slicer): Linear(in_features=16, out_features=8, bias=True)\n",
       "    )\n",
       "    (slice_rorate_half_2): Slicer(\n",
       "      (slicer): Linear(in_features=16, out_features=8, bias=True)\n",
       "    )\n",
       "    (QKV_copier): QKVCopier(\n",
       "      (slicer_0): Slicer(\n",
       "        (slicer): Linear(in_features=1536, out_features=192, bias=True)\n",
       "      )\n",
       "      (slicer_1): Slicer(\n",
       "        (slicer): Linear(in_features=1536, out_features=192, bias=True)\n",
       "      )\n",
       "      (slicer_2): Slicer(\n",
       "        (slicer): Linear(in_features=1536, out_features=192, bias=True)\n",
       "      )\n",
       "      (slicer_3): Slicer(\n",
       "        (slicer): Linear(in_features=1536, out_features=192, bias=True)\n",
       "      )\n",
       "      (slicer_4): Slicer(\n",
       "        (slicer): Linear(in_features=1536, out_features=192, bias=True)\n",
       "      )\n",
       "      (slicer_5): Slicer(\n",
       "        (slicer): Linear(in_features=1536, out_features=192, bias=True)\n",
       "      )\n",
       "      (slicer_6): Slicer(\n",
       "        (slicer): Linear(in_features=1536, out_features=192, bias=True)\n",
       "      )\n",
       "      (slicer_7): Slicer(\n",
       "        (slicer): Linear(in_features=1536, out_features=192, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TOKENS = inps_one_hot.shape[0]\n",
    "attention_mask = torch.ones((1, N_TOKENS), dtype=torch.int)\n",
    "\n",
    "\n",
    "class Slicer(torch.nn.Module):\n",
    "    def __init__(self, inp_dim: int, start_at: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.slicer = torch.nn.Linear(inp_dim, output_dim)\n",
    "        stacked = []\n",
    "        if start_at > 0:\n",
    "            stacked.append(torch.zeros((output_dim, start_at)))\n",
    "        stacked.append(torch.eye(output_dim))\n",
    "        if start_at + output_dim < inp_dim:\n",
    "            stacked.append(torch.zeros((output_dim, inp_dim - output_dim - start_at)))\n",
    "        self.slicer.weight = torch.nn.Parameter(\n",
    "            torch.hstack(stacked)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.slicer(x)\n",
    "\n",
    "class QKVCopier(torch.nn.Module):\n",
    "    def __init__(self, seq_len=N_TOKENS, num_heads=8, hidden_size=64) -> None:\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.slicers = []\n",
    "        # TODO: some magic with dictionary of class\n",
    "        i = 0\n",
    "        self.slicer_0  =Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size)\n",
    "        i = 1\n",
    "        self.slicer_1  =Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size)\n",
    "        i = 2\n",
    "        self.slicer_2  =Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size)\n",
    "        i = 3\n",
    "        self.slicer_3  =Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size)\n",
    "        i = 4\n",
    "        self.slicer_4  =Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size)\n",
    "        i = 5\n",
    "        self.slicer_5  =Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size)\n",
    "        i = 6\n",
    "        self.slicer_6  =Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size)\n",
    "        i = 7\n",
    "        self.slicer_7  =Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size)\n",
    "    \n",
    "        for i in range(num_heads):\n",
    "            self.slicers.append(Slicer(num_heads * 3 * hidden_size, i * 3 * hidden_size, 3 * hidden_size))\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # First we need to shape it to [seq_len, (num_heads * 3 * head_size)]\n",
    "    #     # TODO: use slicer w/ copymat if anything\n",
    "    #     return  [self.slicers[i](x) for i in range(self.num_heads)]\n",
    "\n",
    "class FixedAttentionMask(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attn = model_og.gpt_neox.layers[0].attention\n",
    "        self._init_bias(N_TOKENS)\n",
    "        self.slice_query = Slicer(192, 0, 64)\n",
    "        self.slice_value = Slicer(192, 64, 64)\n",
    "        self.slice_key = Slicer(192, 64 * 2, 64)\n",
    "        self.slice_rotary = Slicer(64, 0, 16)\n",
    "        self.slice_non_rotary = Slicer(64, 16, 64 - 16)\n",
    "        # TODO: I think 16 has to do with having 2 tokens... we need to generalize\n",
    "        self.slice_rorate_half_1 = Slicer(16, 0, 8)\n",
    "        self.slice_rorate_half_2 = Slicer(16, 8, 8)\n",
    "        # QKV_last_size = (N_TOKENS,)\n",
    "        # self.new_qkv_shape = (N_TOKENS, self.attn.num_attention_heads, 3 * self.attn.head_size)\n",
    "        self.QKV_copier = QKVCopier() # TODO: dime sizew\n",
    "        # ATTENTION BIAS ATTENTION BIAS 2 2 2 2 torch.Size([1, 1, 2048, 2048])\n",
    "        # TODO: PARAMETERIZE BETTER\n",
    "        # self.attn_bias_slice_a = Slicer(2048, 0, 2)\n",
    "        # self.attn_bias_slice_b = Slicer(2048, 0, 2)\n",
    "        # self.attn.bias = self.attn.bias[:, :, :2, :2]\n",
    "    \n",
    "    def _init_bias(self, max_positions, device=None):\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                max_positions, max_positions\n",
    "            ),\n",
    "            persistent=False,\n",
    "        )\n",
    "        if device is not None:\n",
    "            self.bias = self.bias.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # TODO: MAKE THIS NON BATCH BASED\n",
    "        # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n",
    "        # compute causal mask from causal mask buffer\n",
    "        query_length, attn_head_size = query.size()\n",
    "        key_length = key.size(-2)\n",
    "\n",
    "        # dynamically increase the causal mask with the key length, if needed.\n",
    "        if key_length > self.bias.shape[-1]:\n",
    "            self._init_bias(N_TOKENS, device=key.device)\n",
    "        # causal_mask = self.attn.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "        causal_mask = self.bias\n",
    "        # A cheap way of doing the above: TODO: MAYBE HARDCODE THIS IN!\n",
    "\n",
    "        # query = query.reshape(num_attention_heads, query_length, attn_head_size)\n",
    "        # key = key.reshape(num_attention_heads, key_length, attn_head_size)\n",
    "        # query = query.view(num_attention_heads, query_length, attn_head_size)\n",
    "        # key = key.view(num_attention_heads, key_length, attn_head_size)\n",
    "        attn_scores = torch.zeros(\n",
    "            query_length,\n",
    "            key_length,\n",
    "            dtype=query.dtype,\n",
    "            device=key.device,\n",
    "        )\n",
    "        # print(\"ATTENTION BIAS\", key_length, query_length, key_length, key_length, self.bias.shape)\n",
    "        attn_scores = attn_scores + (query @ key.transpose(0, 1)) * self.attn.norm_factor\n",
    "        # attn_scores = torch.baddbmm(\n",
    "        #     attn_scores,\n",
    "        #     query,\n",
    "        #     key.transpose(0, 1),\n",
    "        #     beta=1.0,\n",
    "        #     alpha=self.attn.norm_factor,\n",
    "        # )\n",
    "        # print(\"ATTTN SCORES\", attn_scores.shape, attn_scores)\n",
    "        attn_scores = attn_scores.reshape(query_length, key_length)\n",
    "\n",
    "        mask_value = torch.finfo(attn_scores.dtype).min\n",
    "        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "        mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n",
    "        attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = attn_weights.to(value.dtype)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        # todo: put back in\n",
    "        # attn_weights = self.attn.attention_dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: IDK IF THIS IS RIGHT for POSITION IDS or ATTENTION MASK\n",
    "        position_ids=torch.arange(N_TOKENS)\n",
    "        # Compute QKV\n",
    "        # Attention heads [seq_len, hidden_size] --> [seq_len, (np * 3 * head_size)]\n",
    "        qkv = self.attn.query_key_value(x)\n",
    "        print(qkv.shape)\n",
    "\n",
    "        # [seq_len, (num_heads * 3 * head_size)] --> [num_heads, num_tokens, 3 * head_size]\n",
    "        print(\"QKV SIZE\", qkv.size(), qkv.size()[:-1])\n",
    "        # TODO: I think that this has to be a parameter\n",
    "        # qkv = qkv.reshape(*self.new_qkv_shape)\n",
    "        # List of qkv tensors\n",
    "        # qkv = self.QKV_copier(qkv)\n",
    "        # [seq_len, num_attention_heads, 3 * head_size] --> 3 [num_attention_heads, seq_len, head_size]\n",
    "        # query = qkv[..., :self.attn.head_size].permute(1, 0, 2)\n",
    "\n",
    "        def rotate_half(x):\n",
    "            \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "            print(\"ROTATE HALF SLICE DIMS\", x.shape)\n",
    "            # x1 = x[..., : x.shape[-1] // 2]\n",
    "            # x2 = x[..., x.shape[-1] // 2 :]\n",
    "            x1 = self.slice_rorate_half_1(x)\n",
    "            x2 = self.slice_rorate_half_2(x)\n",
    "            return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "        def apply_rotary_embed(q, k, cos, sin, position_ids):\n",
    "            cos = cos[position_ids]\n",
    "            sin = sin[position_ids]\n",
    "            q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "            k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "            return q_embed, k_embed\n",
    " \n",
    "        attn_outputs = []\n",
    "        # TODO: we may have to unroll UHUHUH\n",
    "        for i in range(self.attn.num_attention_heads):\n",
    "            # _qkv = self.QKV_copier.slicers[i](qkv)\n",
    "            lin_layer = self.QKV_copier._modules[f\"slicer_{i}\"]\n",
    "            # _qkv = self.QKV_copier.slicer_0(qkv)\n",
    "            _qkv = lin_layer(qkv)\n",
    "            print(\"LOCAL\", _qkv.shape)\n",
    "            query = self.slice_query(_qkv)\n",
    "            key = self.slice_key(_qkv)\n",
    "            value = self.slice_value(_qkv)\n",
    "\n",
    "            # Compute rotary embeddings on rotary_ndims\n",
    "            # 16 and 64\n",
    "            #query_rot = query[..., :self.attn.rotary_ndims]\n",
    "            #query_pass = query[..., self.attn.rotary_ndims:]\n",
    "            #key_rot = key[..., :self.attn.rotary_ndims]\n",
    "            #key_pass = key[..., self.attn.rotary_ndims:]\n",
    "            # print(\"LOCAL QKV\", query.shape, key.shape, value.shape)\n",
    "            query_rot = self.slice_rotary(query)\n",
    "            query_pass = self.slice_non_rotary(query)\n",
    "            key_rot = self.slice_rotary(key)\n",
    "            key_pass = self.slice_non_rotary(key)\n",
    "\n",
    "            # Compute token offset for rotary embeddings (when decoding)\n",
    "            # print(\"SEQ LEN\", key.shape)\n",
    "            seq_len = key.shape[-2]\n",
    "            cos, sin = self.attn.rotary_emb(value, seq_len=seq_len)\n",
    "            print(query_rot.shape, key_rot.shape, cos.shape, sin.shape, position_ids.shape)\n",
    "            query, key = apply_rotary_embed(query_rot, key_rot, cos, sin, position_ids)\n",
    "            # TODO: idk if cat is OKAY\n",
    "            query = torch.cat((query, query_pass), dim=-1)\n",
    "            key = torch.cat((key, key_pass), dim=-1)\n",
    "\n",
    "            # Cache QKV values\n",
    "            # if has_layer_past:\n",
    "            #     past_key = layer_past[0]\n",
    "            #     past_value = layer_past[1]\n",
    "            #     key = torch.cat((past_key, key), dim=-2)\n",
    "            #     value = torch.cat((past_value, value), dim=-2)\n",
    "            present = None\n",
    "    \n",
    "            # Compute attention\n",
    "            attn_output, attn_weights = self._attn(query, key, value, attention_mask, None)\n",
    "            print(\"ATTN OUTPUT\", attn_output.shape, query.shape, key.shape, value.shape)\n",
    "            attn_outputs.append(attn_output)   \n",
    "    \n",
    "        # Reshape outputs\n",
    "        # attn_output = self.attn._merge_heads(attn_output, self.attn.num_attention_heads, self.attn.head_size)\n",
    "        attn_output = torch.cat(attn_outputs, dim=-1)\n",
    "        print(\"ATTN OUTPUT\", attn_output.shape, attn_outputs[0].shape)\n",
    "        attn_output = self.attn.dense(attn_output)\n",
    "\n",
    "        # outputs = (attn_output, present)\n",
    "        # if output_attentions:\n",
    "        #     outputs += (attn_weights,)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "        return self.attn(x, attention_mask=attention_mask, position_ids=torch.arange(N_TOKENS).unsqueeze(0))\n",
    "\n",
    "\n",
    "embed_linear = torch.nn.Linear(\n",
    "    embd_matrix.shape[0], embd_matrix.shape[1], bias=False)\n",
    "embed_linear.weight = torch.nn.Parameter(embd_matrix.T)\n",
    "\n",
    "\n",
    "class SimplfiedLayerNorm(torch.nn.Module):\n",
    "    def __init__(self, layernorm: torch.nn.LayerNorm) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = layernorm.weight\n",
    "        self.bias = layernorm.bias\n",
    "        self.eps = torch.nn.Linear(512, 512)\n",
    "        self.eps.weight = torch.nn.Parameter(torch.eye(512))\n",
    "        self.eps.bias = torch.nn.Parameter(torch.ones(512) * 1e-5)\n",
    "        # TODO: not constant\n",
    "        self.ones_linear = torch.nn.Linear(512, 512, bias=False)\n",
    "        self.ones_linear.weight = torch.nn.Parameter(torch.ones((512, 512)))\n",
    "        self.ones_linear_neg = torch.nn.Linear(512, 512, bias=False)\n",
    "        self.ones_linear_neg.weight = torch.nn.Parameter(-1 * torch.ones((512, 512)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: I think that this can be made more efficient\n",
    "        expectation_neg = self.ones_linear_neg(x)\n",
    "        variance = self.ones_linear((((x + expectation_neg) * (x + expectation_neg))))\n",
    "        radical = self.eps(variance)\n",
    "        denom = torch.sqrt(radical)\n",
    "        x = x + expectation_neg\n",
    "        # TODO: DENOM HAS PROVLEMS\n",
    "        return x\n",
    "        x = x / denom\n",
    "        return x\n",
    "        x = x * self.weight\n",
    "        x = x + self.bias\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModelSel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed_linear = embed_linear\n",
    "        self.layer_norm = SimplfiedLayerNorm(model_og.gpt_neox.layers[0].input_layernorm)\n",
    "        self.attn = FixedAttentionMask()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed_linear(x)\n",
    "        x = self.layer_norm(x)\n",
    "        # HRMM Unsqueeze no good\n",
    "        # x = x.reshape((1, *x.shape))\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "# G\n",
    "# # TODO: add residuals?\n",
    "# model_sel = torch.nn.Sequential(\n",
    "#     # model_og.gpt_neox.embed_in,\n",
    "#     embed_linear,\n",
    "#     # model_og.gpt_neox.emb_dropout, # we have p = 0.0 and thus useless\n",
    "#     model_og.gpt_neox.layers[0].input_layernorm,\n",
    "#     FixedAttentionMask(),\n",
    "#     # TODO: VERIFY THIS JAZZ\n",
    "# )\n",
    "\n",
    "\n",
    "model_sel = ModelSel()\n",
    "model_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1536])\n",
      "QKV SIZE torch.Size([2, 1536]) torch.Size([2])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'slicer_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2651/4064783108.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minps_one_hot_formatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minps_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# embed_linear.forward(inps_one_hot).shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_sel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minps_one_hot_formatted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2651/613422003.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# HRMM Unsqueeze no good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# x = x.reshape((1, *x.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2651/613422003.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;31m# _qkv = self.QKV_copier.slicers[i](qkv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mlin_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQKV_copier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"slicer_{i}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0;31m# _qkv = self.QKV_copier.slicer_0(qkv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0m_qkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'slicer_0'"
     ]
    }
   ],
   "source": [
    "inps_one_hot.shape, embed_linear.weight.shape\n",
    "inps_one_hot_formatted = inps_one_hot.float()\n",
    "# embed_linear.forward(inps_one_hot).shape\n",
    "model_sel(inps_one_hot_formatted).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1536])\n",
      "QKV SIZE torch.Size([2, 1536]) torch.Size([2])\n",
      "LOCAL torch.Size([2, 192])\n",
      "torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ATTN OUTPUT torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64])\n",
      "LOCAL torch.Size([2, 192])\n",
      "torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ATTN OUTPUT torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64])\n",
      "LOCAL torch.Size([2, 192])\n",
      "torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ATTN OUTPUT torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64])\n",
      "LOCAL torch.Size([2, 192])\n",
      "torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ATTN OUTPUT torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64])\n",
      "LOCAL torch.Size([2, 192])\n",
      "torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ATTN OUTPUT torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64])\n",
      "LOCAL torch.Size([2, 192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:557: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/tmp/ipykernel_2651/2010683214.py:84: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if key_length > self.bias.shape[-1]:\n",
      "/tmp/ipykernel_2651/2010683214.py:115: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ATTN OUTPUT torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64])\n",
      "LOCAL torch.Size([2, 192])\n",
      "torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ATTN OUTPUT torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64])\n",
      "LOCAL torch.Size([2, 192])\n",
      "torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([2, 16])\n",
      "ATTN OUTPUT torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2, 64])\n",
      "ATTN OUTPUT torch.Size([2, 512]) torch.Size([2, 64])\n",
      "Exported graph: graph(%onnx::MatMul_0 : Float(2, 50304, strides=[50304, 1], requires_grad=0, device=cpu),\n",
      "      %attn.attn.query_key_value.weight : Float(1536, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %attn.attn.query_key_value.bias : Float(1536, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.attn.dense.weight : Float(512, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %attn.attn.dense.bias : Float(512, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_query.slicer.weight : Float(64, 192, strides=[192, 1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_query.slicer.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_value.slicer.weight : Float(64, 192, strides=[192, 1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_value.slicer.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_key.slicer.weight : Float(64, 192, strides=[192, 1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_key.slicer.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_rotary.slicer.weight : Float(16, 64, strides=[64, 1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_rotary.slicer.bias : Float(16, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_non_rotary.slicer.weight : Float(48, 64, strides=[64, 1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_non_rotary.slicer.bias : Float(48, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_rorate_half_1.slicer.weight : Float(8, 16, strides=[16, 1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_rorate_half_1.slicer.bias : Float(8, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_rorate_half_2.slicer.weight : Float(8, 16, strides=[16, 1], requires_grad=1, device=cpu),\n",
      "      %attn.slice_rorate_half_2.slicer.bias : Float(8, strides=[1], requires_grad=1, device=cpu),\n",
      "      %attn.QKV_copier.slicer_0.slicer.weight : Float(192, 1536, strides=[1536, 1], requires_grad=1, device=cpu),\n",
      "      %attn.QKV_copier.slicer_0.slicer.bias : Float(192, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::MatMul_84 : Float(50304, 512, strides=[512, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_85 : Float(512, 512, strides=[1, 512], requires_grad=0, device=cpu)):\n",
      "  %/embed_linear/MatMul_output_0 : Float(2, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::MatMul[onnx_name=\"/embed_linear/MatMul\"](%onnx::MatMul_0, %onnx::MatMul_84), scope: __main__.ModelSel::/torch.nn.modules.linear.Linear::embed_linear # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/layer_norm/ones_linear_neg/MatMul_output_0 : Float(2, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::MatMul[onnx_name=\"/layer_norm/ones_linear_neg/MatMul\"](%/embed_linear/MatMul_output_0, %onnx::MatMul_85), scope: __main__.ModelSel::/__main__.SimplfiedLayerNorm::layer_norm/torch.nn.modules.linear.Linear::ones_linear_neg # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/layer_norm/Add_output_0 : Float(2, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/layer_norm/Add\"](%/embed_linear/MatMul_output_0, %/layer_norm/ones_linear_neg/MatMul_output_0), scope: __main__.ModelSel::/__main__.SimplfiedLayerNorm::layer_norm # /tmp/ipykernel_2651/2010683214.py:255:0\n",
      "  %/attn/query_key_value/Gemm_output_0 : Float(2, 1536, strides=[1536, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/query_key_value/Gemm\"](%/layer_norm/Add_output_0, %attn.attn.query_key_value.weight, %attn.attn.query_key_value.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/torch.nn.modules.linear.Linear::query_key_value # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slicer_0/slicer/Gemm_output_0 : Float(2, 192, strides=[192, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slicer_0/slicer/Gemm\"](%/attn/query_key_value/Gemm_output_0, %attn.QKV_copier.slicer_0.slicer.weight, %attn.QKV_copier.slicer_0.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slicer_0/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_query/slicer/Gemm_output_0 : Float(2, 64, strides=[64, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_query/slicer/Gemm\"](%/attn/slicer_0/slicer/Gemm_output_0, %attn.slice_query.slicer.weight, %attn.slice_query.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_query/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_key/slicer/Gemm_output_0 : Float(2, 64, strides=[64, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_key/slicer/Gemm\"](%/attn/slicer_0/slicer/Gemm_output_0, %attn.slice_key.slicer.weight, %attn.slice_key.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_key/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_value/slicer/Gemm_output_0 : Float(2, 64, strides=[64, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_value/slicer/Gemm\"](%/attn/slicer_0/slicer/Gemm_output_0, %attn.slice_value.slicer.weight, %attn.slice_value.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_value/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_rotary/slicer/Gemm_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_rotary/slicer/Gemm\"](%/attn/slice_query/slicer/Gemm_output_0, %attn.slice_rotary.slicer.weight, %attn.slice_rotary.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_rotary/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_non_rotary/slicer/Gemm_output_0 : Float(2, 48, strides=[48, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_non_rotary/slicer/Gemm\"](%/attn/slice_query/slicer/Gemm_output_0, %attn.slice_non_rotary.slicer.weight, %attn.slice_non_rotary.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_non_rotary/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_rotary/slicer_1/Gemm_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_rotary/slicer_1/Gemm\"](%/attn/slice_key/slicer/Gemm_output_0, %attn.slice_rotary.slicer.weight, %attn.slice_rotary.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_rotary/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_non_rotary/slicer_1/Gemm_output_0 : Float(2, 48, strides=[48, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_non_rotary/slicer_1/Gemm\"](%/attn/slice_key/slicer/Gemm_output_0, %attn.slice_non_rotary.slicer.weight, %attn.slice_non_rotary.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_non_rotary/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/Constant_output_0 : Float(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/attn/Constant\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:166:0\n",
      "  %/attn/Mul_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Mul[onnx_name=\"/attn/Mul\"](%/attn/slice_rotary/slicer/Gemm_output_0, %/attn/Constant_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:166:0\n",
      "  %/attn/slice_rorate_half_1/slicer/Gemm_output_0 : Float(2, 8, strides=[8, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_rorate_half_1/slicer/Gemm\"](%/attn/slice_rotary/slicer/Gemm_output_0, %attn.slice_rorate_half_1.slicer.weight, %attn.slice_rorate_half_1.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_rorate_half_1/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_rorate_half_2/slicer/Gemm_output_0 : Float(2, 8, strides=[8, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_rorate_half_2/slicer/Gemm\"](%/attn/slice_rotary/slicer/Gemm_output_0, %attn.slice_rorate_half_2.slicer.weight, %attn.slice_rorate_half_2.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_rorate_half_2/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/Neg_output_0 : Float(2, 8, strides=[8, 1], requires_grad=1, device=cpu) = onnx::Neg[onnx_name=\"/attn/Neg\"](%/attn/slice_rorate_half_2/slicer/Gemm_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:161:0\n",
      "  %/attn/Concat_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=-1, onnx_name=\"/attn/Concat\"](%/attn/Neg_output_0, %/attn/slice_rorate_half_1/slicer/Gemm_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:161:0\n",
      "  %/attn/Constant_1_output_0 : Float(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/attn/Constant_1\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:166:0\n",
      "  %/attn/Mul_1_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Mul[onnx_name=\"/attn/Mul_1\"](%/attn/Concat_output_0, %/attn/Constant_1_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:166:0\n",
      "  %/attn/Add_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/attn/Add\"](%/attn/Mul_output_0, %/attn/Mul_1_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:166:0\n",
      "  %/attn/Constant_2_output_0 : Float(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/attn/Constant_2\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:167:0\n",
      "  %/attn/Mul_2_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Mul[onnx_name=\"/attn/Mul_2\"](%/attn/slice_rotary/slicer_1/Gemm_output_0, %/attn/Constant_2_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:167:0\n",
      "  %/attn/slice_rorate_half_1/slicer_1/Gemm_output_0 : Float(2, 8, strides=[8, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_rorate_half_1/slicer_1/Gemm\"](%/attn/slice_rotary/slicer_1/Gemm_output_0, %attn.slice_rorate_half_1.slicer.weight, %attn.slice_rorate_half_1.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_rorate_half_1/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/slice_rorate_half_2/slicer_1/Gemm_output_0 : Float(2, 8, strides=[8, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/slice_rorate_half_2/slicer_1/Gemm\"](%/attn/slice_rotary/slicer_1/Gemm_output_0, %attn.slice_rorate_half_2.slicer.weight, %attn.slice_rorate_half_2.slicer.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/__main__.Slicer::slice_rorate_half_2/torch.nn.modules.linear.Linear::slicer # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %/attn/Neg_1_output_0 : Float(2, 8, strides=[8, 1], requires_grad=1, device=cpu) = onnx::Neg[onnx_name=\"/attn/Neg_1\"](%/attn/slice_rorate_half_2/slicer_1/Gemm_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:161:0\n",
      "  %/attn/Concat_1_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=-1, onnx_name=\"/attn/Concat_1\"](%/attn/Neg_1_output_0, %/attn/slice_rorate_half_1/slicer_1/Gemm_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:161:0\n",
      "  %/attn/Constant_3_output_0 : Float(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/attn/Constant_3\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:167:0\n",
      "  %/attn/Mul_3_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Mul[onnx_name=\"/attn/Mul_3\"](%/attn/Concat_1_output_0, %/attn/Constant_3_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:167:0\n",
      "  %/attn/Add_1_output_0 : Float(2, 16, strides=[16, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/attn/Add_1\"](%/attn/Mul_2_output_0, %/attn/Mul_3_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:167:0\n",
      "  %/attn/Concat_2_output_0 : Float(2, 64, strides=[64, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=-1, onnx_name=\"/attn/Concat_2\"](%/attn/Add_output_0, %/attn/slice_non_rotary/slicer/Gemm_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:199:0\n",
      "  %/attn/Concat_3_output_0 : Float(2, 64, strides=[64, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=-1, onnx_name=\"/attn/Concat_3\"](%/attn/Add_1_output_0, %/attn/slice_non_rotary/slicer_1/Gemm_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:200:0\n",
      "  %/attn/Constant_4_output_0 : Bool(2, 2, strides=[2, 1], requires_grad=0, device=cpu) = onnx::Constant[value= 1  0  1  1 [ CPUBoolType{2,2} ], onnx_name=\"/attn/Constant_4\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:84:0\n",
      "  %/attn/Constant_5_output_0 : Long(2, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 2  2 [ CPULongType{2} ], onnx_name=\"/attn/Constant_5\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:110:0\n",
      "  %/attn/Transpose_output_0 : Float(64, 2, strides=[1, 64], requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 0], onnx_name=\"/attn/Transpose\"](%/attn/Concat_3_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:101:0\n",
      "  %/attn/MatMul_output_0 : Float(2, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::MatMul[onnx_name=\"/attn/MatMul\"](%/attn/Concat_2_output_0, %/attn/Transpose_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:101:0\n",
      "  %/attn/Constant_6_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name=\"/attn/Constant_6\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:101:0\n",
      "  %/attn/Mul_4_output_0 : Float(2, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Mul[onnx_name=\"/attn/Mul_4\"](%/attn/MatMul_output_0, %/attn/Constant_6_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:101:0\n",
      "  %/attn/Constant_7_output_0 : Float(2, 2, strides=[2, 1], requires_grad=0, device=cpu) = onnx::Constant[value= 0  0  0  0 [ CPUFloatType{2,2} ], onnx_name=\"/attn/Constant_7\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:101:0\n",
      "  %/attn/Add_2_output_0 : Float(2, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/attn/Add_2\"](%/attn/Constant_7_output_0, %/attn/Mul_4_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:101:0\n",
      "  %/attn/Reshape_output_0 : Float(2, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/attn/Reshape\"](%/attn/Add_2_output_0, %/attn/Constant_5_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:110:0\n",
      "  %onnx::Where_75 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={-3.40282e+38}]()\n",
      "  %/attn/Where_output_0 : Float(2, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Where[onnx_name=\"/attn/Where\"](%/attn/Constant_4_output_0, %/attn/Reshape_output_0, %onnx::Where_75), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:116:0\n",
      "  %/attn/Constant_8_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cpu) = onnx::Constant[value= 1  1 [ CPUFloatType{1,2} ], onnx_name=\"/attn/Constant_8\"](), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:120:0\n",
      "  %/attn/Add_3_output_0 : Float(2, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/attn/Add_3\"](%/attn/Where_output_0, %/attn/Constant_8_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:120:0\n",
      "  %/attn/Softmax_output_0 : Float(2, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Softmax[axis=1, onnx_name=\"/attn/Softmax\"](%/attn/Add_3_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /home/lev/.local/lib/python3.10/site-packages/torch/nn/functional.py:1858:0\n",
      "  %/attn/Cast_output_0 : Float(2, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Cast[to=1, onnx_name=\"/attn/Cast\"](%/attn/Softmax_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:123:0\n",
      "  %/attn/MatMul_1_output_0 : Float(2, 64, strides=[64, 1], requires_grad=1, device=cpu) = onnx::MatMul[onnx_name=\"/attn/MatMul_1\"](%/attn/Cast_output_0, %/attn/slice_value/slicer/Gemm_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:132:0\n",
      "  %/attn/Concat_4_output_0 : Float(2, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=-1, onnx_name=\"/attn/Concat_4\"](%/attn/MatMul_1_output_0, %/attn/MatMul_1_output_0, %/attn/MatMul_1_output_0, %/attn/MatMul_1_output_0, %/attn/MatMul_1_output_0, %/attn/MatMul_1_output_0, %/attn/MatMul_1_output_0, %/attn/MatMul_1_output_0), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn # /tmp/ipykernel_2651/2010683214.py:217:0\n",
      "  %83 : Float(2, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/attn/dense/Gemm\"](%/attn/Concat_4_output_0, %attn.attn.dense.weight, %attn.attn.dense.bias), scope: __main__.ModelSel::/__main__.FixedAttentionMask::attn/torch.nn.modules.linear.Linear::dense # /home/lev/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  return (%83)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inps_one_hot_formatted = inps_one_hot.float()\n",
    "torch.onnx.export(model_sel, inps_one_hot_formatted,\n",
    "                  'model_sel.onnx', verbose=True, opset_version=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Names: ['onnx::MatMul_0']\n",
      "Output Names: ['134']\n",
      "0 Identity_14\n",
      "1 Identity_15\n",
      "2 Identity_16\n",
      "3 Identity_17\n",
      "4 /embed_linear/MatMul\n",
      "5 /layer_norm/ones_linear_neg/MatMul\n",
      "6 /layer_norm/Add\n",
      "7 /attn/query_key_value/Gemm\n",
      "8 /attn/Constant\n",
      "9 /attn/Reshape\n",
      "10 /attn/slice_query/slicer/MatMul\n",
      "11 /attn/slice_query/slicer/Add\n",
      "12 /attn/Transpose\n",
      "13 /attn/slice_key/slicer/MatMul\n",
      "14 /attn/slice_key/slicer/Add\n",
      "15 /attn/Transpose_1\n",
      "16 /attn/slice_value/slicer/MatMul\n",
      "17 /attn/slice_value/slicer/Add\n",
      "18 /attn/Transpose_2\n",
      "19 /attn/slice_rotary/slicer/MatMul\n",
      "20 /attn/slice_rotary/slicer/Add\n",
      "21 /attn/slice_non_rotary/slicer/MatMul\n",
      "22 /attn/slice_non_rotary/slicer/Add\n",
      "23 /attn/slice_rotary/slicer_1/MatMul\n",
      "24 /attn/slice_rotary/slicer_1/Add\n",
      "25 /attn/slice_non_rotary/slicer_1/MatMul\n",
      "26 /attn/slice_non_rotary/slicer_1/Add\n",
      "27 /attn/Constant_1\n",
      "28 /attn/Mul\n",
      "29 /attn/slice_rorate_half_1/slicer/MatMul\n",
      "30 /attn/slice_rorate_half_1/slicer/Add\n",
      "31 /attn/slice_rorate_half_2/slicer/MatMul\n",
      "32 /attn/slice_rorate_half_2/slicer/Add\n",
      "33 /attn/Neg\n",
      "34 /attn/Concat\n",
      "35 /attn/Constant_2\n",
      "36 /attn/Mul_1\n",
      "37 /attn/Add\n",
      "38 /attn/Constant_3\n",
      "39 /attn/Mul_2\n",
      "40 /attn/slice_rorate_half_1/slicer_1/MatMul\n",
      "41 /attn/slice_rorate_half_1/slicer_1/Add\n",
      "42 /attn/slice_rorate_half_2/slicer_1/MatMul\n",
      "43 /attn/slice_rorate_half_2/slicer_1/Add\n",
      "44 /attn/Neg_1\n",
      "45 /attn/Concat_1\n",
      "46 /attn/Constant_4\n",
      "47 /attn/Mul_3\n",
      "48 /attn/Add_1\n",
      "49 /attn/Concat_2\n",
      "50 /attn/Concat_3\n",
      "51 /attn/Constant_5\n",
      "52 /attn/Constant_6\n",
      "53 /attn/Reshape_1\n",
      "54 /attn/Constant_7\n",
      "55 /attn/Reshape_2\n",
      "56 /attn/Constant_8\n",
      "57 /attn/Transpose_3\n",
      "58 /attn/MatMul\n",
      "59 /attn/Constant_9\n",
      "60 /attn/Mul_4\n",
      "61 /attn/Constant_10\n",
      "62 /attn/Add_2\n",
      "63 /attn/Reshape_3\n",
      "64 Constant_78\n",
      "65 /attn/Where\n",
      "66 /attn/Constant_11\n",
      "67 /attn/Add_3\n",
      "68 /attn/Softmax\n",
      "69 /attn/Cast\n",
      "70 /attn/MatMul_1\n",
      "71 /attn/Transpose_4\n",
      "72 /attn/Constant_12\n",
      "73 /attn/Reshape_4\n",
      "74 /attn/dense/MatMul\n",
      "75 /attn/dense/Add\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"model_sel.onnx\")\n",
    "\n",
    "# Get the names of input and output nodes\n",
    "input_names = [input.name for input in model.graph.input]\n",
    "output_names = [output.name for output in model.graph.output]\n",
    "\n",
    "print(\"Input Names:\", input_names)\n",
    "print(\"Output Names:\", output_names)\n",
    "\n",
    "model_onnx = onnx.load('model_sel.onnx')\n",
    "for i, node in enumerate(model.graph.node):\n",
    "    if node.op_type == 'Gather':\n",
    "        print(f\"'Gather' found in node {i}: {node.name}\")\n",
    "    else:\n",
    "        print(i, node.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+git://github.com/onnx/onnx.git@master\n",
      "  Cloning git://github.com/onnx/onnx.git (to revision master) to /tmp/pip-req-build-v5b89tqm\n",
      "  Running command git clone --filter=blob:none --quiet git://github.com/onnx/onnx.git /tmp/pip-req-build-v5b89tqm\n",
      "  fatal: unable to connect to github.com:\n",
      "  github.com[0: 2607:7700:0:2a:0:2:8c52:7004]: errno=Connection timed out\n",
      "  github.com[1: 140.82.114.3]: errno=Network is unreachable\n",
      "\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet git:\u001b[0m\u001b[32m/\u001b[0m\u001b[32m/github.com/onnx/\u001b[0m\u001b[32monnx.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-v5b89tqm\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet git:\u001b[0m\u001b[32m/\u001b[0m\u001b[32m/github.com/onnx/\u001b[0m\u001b[32monnx.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-v5b89tqm\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+git://github.com/onnx/onnx.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch2keras\n",
      "  Downloading pytorch2keras-0.2.4.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras\n",
      "  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m362.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/lev/.local/lib/python3.10/site-packages (from pytorch2keras) (1.26.4)\n",
      "Requirement already satisfied: onnx in /home/lev/.local/lib/python3.10/site-packages (from pytorch2keras) (1.15.0)\n",
      "Collecting onnx2keras\n",
      "  Downloading onnx2keras-0.0.24.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/589.8 MB\u001b[0m \u001b[31m411.3 kB/s\u001b[0m eta \u001b[36m0:21:03\u001b[0m^C\n",
      "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/589.8 MB\u001b[0m \u001b[31m407.4 kB/s\u001b[0m eta \u001b[36m0:21:15\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch2keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch2keras.converter import pytorch_to_keras\n",
    "\n",
    "k_model = pytorch_to_keras(model_sel, inps_one_hot, [inps_one_hot.shape], verbose=True, names='short')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Mariboupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=\"$PYTHONPATH:/home/lev/code/research/ai/dictator/Marabou\"\n",
    "# !export PYTHONPATH=\"$PYTHONPATH:/home/lev/code/research/ai/dictator/Marabou/maraboupy\"\n",
    "# !pip install onnx onnxruntime maraboupy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in /home/lev/.local/lib/python3.9/site-packages (1.15.0)\n",
      "Requirement already satisfied: onnxruntime in /home/lev/.local/lib/python3.9/site-packages (1.17.1)\n",
      "Requirement already satisfied: numpy in /home/lev/.local/lib/python3.9/site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/lev/.local/lib/python3.9/site-packages (from onnx) (3.20.3)\n",
      "Requirement already satisfied: coloredlogs in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: packaging in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (23.1)\n",
      "Requirement already satisfied: sympy in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/lev/.local/lib/python3.9/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lev/.local/lib/python3.9/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lev/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "## %\n",
    "# Path to Marabou folder if you did not export it\n",
    "\n",
    "# TODO: this is a hack, fix it\n",
    "# TODO: add Marabou installation details to readme\n",
    "# sys.path.append('/home/lev/code/research/softmax_bound/bounding-softmax/Marabou')\n",
    "sys.path.append('/home/lev/code/research/ai/dictator/Marabou')\n",
    "from maraboupy import Marabou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Attention Head\n"
     ]
    }
   ],
   "source": [
    "options = Marabou.createOptions(verbosity = 1)\n",
    "print(\"Simple Attention Head\")\n",
    "filename = \"model_sel.onnx\"\n",
    "network = Marabou.read_onnx(filename)#, inputNames=inputNames, outputNames=[outputName])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around with basic constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
