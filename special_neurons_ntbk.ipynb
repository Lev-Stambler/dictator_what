{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_neox.layers.0.mlp', 'gpt_neox.layers.0.mlp.dense_h_to_4h')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from special_neurons import get_most_negative_sets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "model_name = 'EleutherAI/pythia-70m'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_og = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "most_neg = get_most_negative_sets(model_og)\n",
    "most_neg[0].prev_layer_name, most_neg[0].linear_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel_og\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           GPTNeoXForCausalLM\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "GPTNeoXForCausalLM(\n",
      "           (gpt_neox): GPTNeoXModel(\n",
      "           (embed_in): Embedding(50304, 512)\n",
      "           (emb_dr <...> entwise_affine=True)\n",
      "           )\n",
      "           (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      "           )\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "GPTNeoX Model with a `language modeling` head on top for CLM fine-tuning.\n",
      "This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n",
      "it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
      "behavior.\n",
      "\n",
      "Parameters:\n",
      "    config ([`~GPTNeoXConfig`]): Model configuration class with all the parameters of the model.\n",
      "        Initializing with a config file does not load the weights associated with the model, only the\n",
      "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n"
     ]
    }
   ],
   "source": [
    "model_og?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of weights at a specific layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f75c0319390>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from special_neurons import find_linear_layer_pairs, ordered_magnitude_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "linear_layer_pairs = find_linear_layer_pairs(model_og)\n",
    "LAYER_PLOT = 0\n",
    "\n",
    "plt.plot(ordered_magnitude_output(linear_layer_pairs[LAYER_PLOT])[0].flatten().sort()[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the most negative wire settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "gpt_neox.layers.0.mlp\n",
      "torch.Size([1, 44, 2048])\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "\n",
    "def get_weights_from_transformer_lens(name: str):\n",
    "  #  TODO: this is a hack with gpt_ne_x\n",
    "  match = re.match(r\"gpt_neox\\.layers\\.(\\d+)\\.mlp\", name)\n",
    "  if not match:\n",
    "    raise ValueError(f\"Name {name} does not match the expected pattern\")\n",
    "  print(name)\n",
    "  layer_number = int(match.group(1))\n",
    "  hooked_name = f\"blocks.{layer_number}.mlp.hook_post\"\n",
    "  print(activations[hooked_name].shape)\n",
    "  return hooked_name\n",
    "\n",
    "# Load a model (eg GPT-2 Small)\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_name)\n",
    "# Run the model and get logits and activations\n",
    "logits, activations = model.run_with_cache(\"\"\"My name is jackie jacobson and I like to frolic in the\n",
    "\n",
    "the umpires, the umpires, the umpires, the umpires, the umpires\"\"\")\n",
    "# logits, activations = model.run_with_cache(\"Hello world\")\n",
    "\n",
    "# For the first MLP\n",
    "LAYER_CHECK = 2\n",
    "trans_lens_name = get_weights_from_transformer_lens(linear_layer_pairs[LAYER_CHECK].prev_layer_name)\n",
    "weights_layer = linear_layer_pairs[LAYER_CHECK].linear_layer.weight\n",
    "activations_prev_layer = activations[trans_lens_name]\n",
    "activations_prev_layer = activations_prev_layer.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50277"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_vocab_size = len(model.tokenizer)\n",
    "tokenizer_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens = activations_prev_layer.shape[0]\n",
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 512]), torch.Size([44, 2048]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_layer.shape, activations_prev_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distr_outgoing(token: int, is_min=True, sort_out=True):\n",
    "\tmost = float('inf') if is_min else float('-inf')\n",
    "\tmost_idx= -1\n",
    "\tvs = torch.zeros_like(weights_layer[0])\n",
    "\tfor outgoing_wire in range(weights_layer.shape[1]):\n",
    "\t\ttmp = activations_prev_layer[token] * weights_layer.T[outgoing_wire]\n",
    "\t\tv = min(tmp) if is_min else max(tmp)\n",
    "\t\tif (is_min and v < most) or (not is_min and v > most):\n",
    "\t\t\tmost = v\n",
    "\t\t\tmost_idx = outgoing_wire\n",
    "\t\tvs[outgoing_wire] = v\n",
    "\tprint(\"MINS\", most_idx, most)\n",
    "\tif not sort_out:\n",
    "\t\tplt.plot(vs.cpu().detach().numpy())\n",
    "\t\treturn\n",
    "\tif not is_min:\n",
    "\t\tplt.plot(vs.sort()[0].cpu().detach().numpy())\n",
    "\telse: \n",
    "\t\tplt.plot(vs.sort()[0].cpu().detach().numpy()[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINS 295 tensor(-0.1757, grad_fn=<UnbindBackward0>)\n",
      "MINS 160 tensor(0.2371, grad_fn=<UnbindBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "FOR_TOK = 2\n",
    "plot_distr_outgoing(FOR_TOK, is_min=True, sort_out=True)\n",
    "plot_distr_outgoing(FOR_TOK, is_min=False, sort_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR_TOK = n_tokens - 1\n",
    "# plot_distr_outgoing(FOR_TOK, is_min=True, sort_out=False)\n",
    "# plot_distr_outgoing(FOR_TOK, is_min=False, sort_out=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the distribution of weights on the attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1700)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(activations['blocks.1.mlp.hook_post'][0][0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of negative activations: 0.1669921875\n"
     ]
    }
   ],
   "source": [
    "prop_off = 1  - (activations['blocks.1.mlp.hook_post'][0][0] > -0.16).nonzero().shape[0] / 2048\n",
    "\n",
    "print(f\"Proportion of negative activations: {prop_off}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[9.9983084e-01 1.6914749e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "[9.2802393e-01 2.2540744e-05 4.0183222e-05 5.4520939e-02 1.0564207e-02\n",
      " 4.0690538e-06 8.2021195e-04 5.1404874e-07 3.9897554e-07 9.9081488e-04\n",
      " 5.0121555e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "[6.91128448e-02 4.19571444e-09 3.81221832e-10 6.47311538e-09\n",
      " 3.20442428e-09 1.23078285e-08 9.77002420e-12 3.32507605e-10\n",
      " 5.16936355e-11 5.62393998e-09 4.10837356e-06 6.81031720e-09\n",
      " 1.87926627e-12 6.98440639e-10 1.36707225e-11 5.66635697e-11\n",
      " 1.87249882e-10 1.12266385e-09 6.79804302e-08 6.18968912e-08\n",
      " 1.56829216e-07 1.38860043e-11 2.17735803e-07 2.27875961e-03\n",
      " 2.21164640e-07 1.89200387e-06 8.08503750e-11 4.26496961e-04\n",
      " 3.81677374e-02 2.13139606e-06 1.91459203e-05 4.30296798e-10\n",
      " 1.20316958e-03 1.63541824e-01 1.72968976e-05 2.68423355e-05\n",
      " 3.47158235e-09 2.35109520e-03 7.20172405e-01 5.02461262e-05\n",
      " 5.60536027e-05 4.77297579e-09 2.56708730e-03 0.00000000e+00]\n",
      "[1.58144776e-02 1.38684791e-05 7.67502061e-05 6.07325416e-03\n",
      " 1.18837568e-06 3.18600001e-07 6.49808527e-08 1.33572771e-08\n",
      " 3.75968034e-10 2.00516475e-08 7.58786919e-03 1.12310372e-05\n",
      " 3.43765230e-08 1.11202326e-06 3.15456710e-08 8.25892202e-11\n",
      " 1.93647361e-06 1.91391291e-06 5.72591453e-05 1.25554958e-04\n",
      " 1.09510984e-04 1.93647361e-06 6.06781692e-09 1.65934424e-07\n",
      " 9.91727561e-02 9.44325514e-03 9.27455767e-06 6.97350924e-06\n",
      " 1.60539298e-06 5.51979505e-02 2.43544772e-01 2.51125712e-05\n",
      " 1.00281877e-05 2.37261702e-06 4.40076441e-02 2.36051679e-01\n",
      " 6.11905998e-05 2.10644612e-05 2.62624758e-06 3.50859538e-02\n",
      " 2.47380048e-01 5.23390954e-05 4.27179220e-05 2.17723573e-06]\n"
     ]
    }
   ],
   "source": [
    "# activations['blocks.0.attn.hook_attn_scores'].shape\n",
    "print(activations['blocks.4.attn.hook_pattern'][0][1][0].cpu().detach().numpy() )\n",
    "print(activations['blocks.4.attn.hook_pattern'][0][1][1].cpu().detach().numpy() )\n",
    "print(activations['blocks.4.attn.hook_pattern'][0][1][10].cpu().detach().numpy() )\n",
    "print(activations['blocks.4.attn.hook_pattern'][0][1][-2].cpu().detach().numpy() )\n",
    "print(activations['blocks.4.attn.hook_pattern'][0][1][-1].cpu().detach().numpy() )\n",
    "# plt.plot(activations['blocks.0.attn.hook_attn_scores'][0][0][0].cpu().detach().numpy() )\n",
    "# activations['blocks.0.hook_attn_out']\n",
    "# activations['blocks.0.attn.hook_z'].shape\n",
    "# activations['blocks.0.attn.hook_pattern'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2D, but have shapes (8,) and (8, 44, 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3942/2750532150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blocks.0.attn.hook_pattern'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# activations.keys()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2758\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             raise ValueError(f\"x and y can be no greater than 2D, but have \"\n\u001b[0m\u001b[1;32m    502\u001b[0m                              f\"shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y can be no greater than 2D, but have shapes (8,) and (8, 44, 44)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(activations['blocks.0.attn.hook_pattern'][0].cpu().detach().numpy())\n",
    "# activations.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Some Basic SDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install cvxpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if parrallel residuals are used as per line 709 of tranformers/models/gpt_neox/modeling_gpt_neox.py\n",
    "assert model_og.gpt_neox.layers[0].use_parallel_residual == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3942/931520965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                 \u001b[0;31m# Ensure that cross-terms are 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                 \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtokenizer_vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtokenizer_vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/expressions/expression.py\u001b[0m in \u001b[0;36mcast_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \"\"\"\n\u001b[1;32m     49\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_to_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/expressions/expression.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"Equality : Creates a constraint ``self == other``.\n\u001b[1;32m    723\u001b[0m         \"\"\"\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mEquality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_cast_other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/constraints/zero.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lhs, rhs, constr_id)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstr_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlhs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEquality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstr_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/constraints/constraint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, constr_id)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstr_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstr_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dual_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConstraint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/constraints/zero.py\u001b[0m in \u001b[0;36m_construct_dual_variables\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_construct_dual_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEquality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dual_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/constraints/constraint.py\u001b[0m in \u001b[0;36m_construct_dual_variables\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_construct_dual_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcvxtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/constraints/constraint.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_construct_dual_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcvxtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/expressions/variable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, name, var_id, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     ):\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvar_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/cvxpy/lin_ops/lin_utils.py\u001b[0m in \u001b[0;36mget_id\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \"\"\"Returns a new id and updates the id counter.\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "TARGET_TOKEN = 0\n",
    "TARGET_NEURON = 0\n",
    "TARGET_LAYER = 0\n",
    "assert TARGET_LAYER == 0, \"We only support the first layer for now\"\n",
    "# seed = 69_420\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# We have a dimension `tokenizer_vocab_size`-vector for each token\n",
    "n_dim = tokenizer_vocab_size * n_tokens\n",
    "X = cp.Variable((n_dim, n_dim), PSD=True)\n",
    "\n",
    "# Ensure every element is 0 <= x\n",
    "constraints = [X >= 0]\n",
    "\n",
    "# Add each unary-ish constraint\n",
    "for tok in range(n_tokens):\n",
    "\tfor i in range(tokenizer_vocab_size):\n",
    "\t\tfor j in range(tokenizer_vocab_size):\n",
    "\t\t\tidx = tok * tokenizer_vocab_size + i\n",
    "\t\t\tidy = tok * tokenizer_vocab_size + j\n",
    "\t\t\tif i != j:\n",
    "\t\t\t\t# Ensure that cross-terms are 0\n",
    "\t\t\t\tconstraints.append(X[idx, idy] == 0)\n",
    "\tstart_idx = tok * tokenizer_vocab_size\n",
    "\tend_idx = (tok + 1) * tokenizer_vocab_size\n",
    "\tsub_matrix = X[start_idx:end_idx, start_idx:end_idx]\n",
    "\t# Ensure that the sum of squares is 1\n",
    "\tconstraints.append(cp.trace(sub_matrix) == 1)\n",
    "\n",
    "def module_to_np(module: torch.Tensor):\n",
    "\treturn module.cpu().detach().numpy()\n",
    "\n",
    "def build_obj():\n",
    "\t# We want to recreate what the neural network is doing\n",
    "\tembed = np.kron(np.eye(n_tokens), module_to_np(model.embed.W_E))\n",
    "\tprint(embed.shape)\n",
    "\tmodel_og\n",
    "\t# TODO: WHERE IS POSITIONAL EMBEDDING?\n",
    "\tpost_embed = embed @ X\n",
    "\t# TODO: then nn.dropout for embeding\n",
    "\n",
    "\t# TODO: what about residuals???\n",
    "\n",
    "\t# Then, line 671 get GPTNeoXLayer \n",
    "\n",
    "\n",
    "\t\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\n",
    "q = cp.Parameter(n_dim, nonneg=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at most negative individual neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HookedTransformer.forward() got an unexpected keyword argument 'output_hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6407/245202709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Forward pass to get output logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m  \u001b[0;31m# Hidden states of all layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: HookedTransformer.forward() got an unexpected keyword argument 'output_hidden_states'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Replace MODEL_NAME with the actual model name\n",
    "\n",
    "# Encode the input text\n",
    "input_text = \"The boy with the SFdfdrgfeDSSD\"  # Replace YOUR_INPUT_TEXT with your actual input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# Forward pass to get output logits\n",
    "outputs = model(input_ids, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states  # Hidden states of all layers\n",
    "\n",
    "# Access the specific neuron's value\n",
    "# Replace LAYER_INDEX, BATCH_INDEX, TOKEN_INDEX, NEURON_INDEX with actual indices\n",
    "# neuron_value = hidden_states[LAYER_INDEX][BATCH_INDEX, TOKEN_INDEX, NEURON_INDEX].item()\n",
    "\n",
    "print(hidden_states[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer_lens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6407/344226125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformer_lens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load a model (eg GPT-2 Small)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHookedTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run the model and get logits and activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformer_lens'"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "\n",
    "# Load a model (eg GPT-2 Small)\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_name)\n",
    "# Run the model and get logits and activations\n",
    "logits, activations = model.run_with_cache(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.layers.0.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 227\n",
      "Activations: tensor([0.4309], device='cuda:0') on token 1 with negative value -13.506959915161133\n",
      "Effective change tensor([-5.8199], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 249\n",
      "Activations: tensor([0.3862], device='cuda:0') on token 1 with negative value -74.46073150634766\n",
      "Effective change tensor([-28.7561], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.1.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 47\n",
      "Activations: tensor([1.0227], device='cuda:0') on token 1 with negative value -17.482093811035156\n",
      "Effective change tensor([-17.8796], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 157\n",
      "Activations: tensor([0.5185], device='cuda:0') on token 2 with negative value -21.773366928100586\n",
      "Effective change tensor([-11.2893], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.2.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 111\n",
      "Activations: tensor([0.8778], device='cuda:0') on token 0 with negative value -12.92142391204834\n",
      "Effective change tensor([-11.3418], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 156\n",
      "Activations: tensor([0.3742], device='cuda:0') on token 0 with negative value -24.87030601501465\n",
      "Effective change tensor([-9.3067], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 71\n",
      "Activations: tensor([0.5842], device='cuda:0') on token 2 with negative value -12.548407554626465\n",
      "Effective change tensor([-7.3304], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 157\n",
      "Activations: tensor([0.5477], device='cuda:0') on token 2 with negative value -19.010305404663086\n",
      "Effective change tensor([-10.4110], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.3.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 52\n",
      "Activations: tensor([0.6890], device='cuda:0') on token 1 with negative value -12.224004745483398\n",
      "Effective change tensor([-8.4220], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.4.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "gpt_neox.layers.5.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 279\n",
      "Activations: tensor([2.1954], device='cuda:0') on token 0 with negative value -16.212507247924805\n",
      "Effective change tensor([-35.5936], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_MAX_LOOK = 100\n",
    "\n",
    "# Okie, this is what we want!!\n",
    "\n",
    "\n",
    "def find_logits_on_mlps():\n",
    "    for layer_pair in most_neg:\n",
    "        name = layer_pair.prev_layer_name\n",
    "        match = re.match(r\"gpt_neox\\.layers\\.(\\d+)\\.mlp\", name)\n",
    "        if match:\n",
    "            print(name)\n",
    "            layer_number = int(match.group(1))\n",
    "            hooked_name = f\"blocks.{layer_number}.mlp.hook_post\"\n",
    "            print(activations[hooked_name].shape)\n",
    "            # Find the most positive activations\n",
    "            # TODO: this is weird... it has to be **per token**\n",
    "            most_neg_out = layer_pair.most_negatives.tolist()\n",
    "            n_tokens = activations[hooked_name].shape[1]\n",
    "            print(\"N Tokens\", n_tokens)\n",
    "            for tok_idx in range(n_tokens):\n",
    "                maxed = activations[hooked_name][0, tok_idx].argsort(descending=True)[\n",
    "                    :N_MAX_LOOK]\n",
    "                for m in maxed:\n",
    "                    for j in range(len(most_neg_out)):\n",
    "                        if m == most_neg_out[j]:\n",
    "                            print(f\"Found a match: {m}\")\n",
    "                            # TODO: print token\n",
    "                            print(\n",
    "                                f\"Activations: {activations[hooked_name][:, tok_idx, m]} on token {tok_idx} with negative value {layer_pair.most_negatives_vals[j]}\")\n",
    "                            effective_change = layer_pair.most_negatives_vals[j] * activations[hooked_name][:, tok_idx, m]\n",
    "                            print(\"Effective change\", effective_change.item())\n",
    "                            print()\n",
    "\n",
    "\n",
    "find_logits_on_mlps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
