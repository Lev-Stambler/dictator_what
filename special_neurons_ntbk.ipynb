{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_neox.layers.0.mlp', 'gpt_neox.layers.0.mlp.dense_h_to_4h')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from special_neurons import get_most_negative_sets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "model_name = 'EleutherAI/pythia-70m'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "most_neg = get_most_negative_sets(model)\n",
    "most_neg[0].prev_layer_name, most_neg[0].linear_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Replace MODEL_NAME with the actual model name\n",
    "\n",
    "# Encode the input text\n",
    "input_text = \"The boy with the SFdfdrgfeDSSD\"  # Replace YOUR_INPUT_TEXT with your actual input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# Forward pass to get output logits\n",
    "outputs = model(input_ids, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states  # Hidden states of all layers\n",
    "\n",
    "# Access the specific neuron's value\n",
    "# Replace LAYER_INDEX, BATCH_INDEX, TOKEN_INDEX, NEURON_INDEX with actual indices\n",
    "# neuron_value = hidden_states[LAYER_INDEX][BATCH_INDEX, TOKEN_INDEX, NEURON_INDEX].item()\n",
    "\n",
    "print(hidden_states[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "\n",
    "# Load a model (eg GPT-2 Small)\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_name)\n",
    "# Run the model and get logits and activations\n",
    "logits, activations = model.run_with_cache(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.layers.0.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 227\n",
      "Activations: tensor([0.4309], device='cuda:0') on token 1 with negative value -13.506959915161133\n",
      "Effective change tensor([-5.8199], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 249\n",
      "Activations: tensor([0.3862], device='cuda:0') on token 1 with negative value -74.46073150634766\n",
      "Effective change tensor([-28.7561], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.1.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 47\n",
      "Activations: tensor([1.0227], device='cuda:0') on token 1 with negative value -17.482093811035156\n",
      "Effective change tensor([-17.8796], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 157\n",
      "Activations: tensor([0.5185], device='cuda:0') on token 2 with negative value -21.773366928100586\n",
      "Effective change tensor([-11.2893], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.2.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 111\n",
      "Activations: tensor([0.8778], device='cuda:0') on token 0 with negative value -12.92142391204834\n",
      "Effective change tensor([-11.3418], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 156\n",
      "Activations: tensor([0.3742], device='cuda:0') on token 0 with negative value -24.87030601501465\n",
      "Effective change tensor([-9.3067], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 71\n",
      "Activations: tensor([0.5842], device='cuda:0') on token 2 with negative value -12.548407554626465\n",
      "Effective change tensor([-7.3304], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 157\n",
      "Activations: tensor([0.5477], device='cuda:0') on token 2 with negative value -19.010305404663086\n",
      "Effective change tensor([-10.4110], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.3.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 52\n",
      "Activations: tensor([0.6890], device='cuda:0') on token 1 with negative value -12.224004745483398\n",
      "Effective change tensor([-8.4220], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.4.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "gpt_neox.layers.5.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 279\n",
      "Activations: tensor([2.1954], device='cuda:0') on token 0 with negative value -16.212507247924805\n",
      "Effective change tensor([-35.5936], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_MAX_LOOK = 100\n",
    "\n",
    "# Okie, this is what we want!!\n",
    "\n",
    "\n",
    "def find_logits_on_mlps():\n",
    "    for layer_pair in most_neg:\n",
    "        name = layer_pair.prev_layer_name\n",
    "        match = re.match(r\"gpt_neox\\.layers\\.(\\d+)\\.mlp\", name)\n",
    "        if match:\n",
    "            print(name)\n",
    "            layer_number = int(match.group(1))\n",
    "            hooked_name = f\"blocks.{layer_number}.mlp.hook_post\"\n",
    "            print(activations[hooked_name].shape)\n",
    "            # Find the most positive activations\n",
    "            # TODO: this is weird... it has to be **per token**\n",
    "            most_neg_out = layer_pair.most_negatives.tolist()\n",
    "            n_tokens = activations[hooked_name].shape[1]\n",
    "            print(\"N Tokens\", n_tokens)\n",
    "            for tok_idx in range(n_tokens):\n",
    "                maxed = activations[hooked_name][0, tok_idx].argsort(descending=True)[\n",
    "                    :N_MAX_LOOK]\n",
    "                for m in maxed:\n",
    "                    for j in range(len(most_neg_out)):\n",
    "                        if m == most_neg_out[j]:\n",
    "                            print(f\"Found a match: {m}\")\n",
    "                            # TODO: print token\n",
    "                            print(\n",
    "                                f\"Activations: {activations[hooked_name][:, tok_idx, m]} on token {tok_idx} with negative value {layer_pair.most_negatives_vals[j]}\")\n",
    "                            effective_change = layer_pair.most_negatives_vals[j] * activations[hooked_name][:, tok_idx, m]\n",
    "                            print(\"Effective change\", effective_change.item())\n",
    "                            print()\n",
    "\n",
    "\n",
    "find_logits_on_mlps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
