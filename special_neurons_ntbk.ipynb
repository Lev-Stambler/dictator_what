{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448fdc346b2c4c7cbc746cf19e57c032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1909c2f3808a4cae90091b674b2127d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('gpt_neox.layers.0.mlp', 'gpt_neox.layers.0.mlp.dense_h_to_4h')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from special_neurons import get_most_negative_sets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "model_name = 'EleutherAI/pythia-70m'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "most_neg = get_most_negative_sets(model)\n",
    "most_neg[0].prev_layer_name, most_neg[0].linear_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of weights at a specific layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4bdb629c30>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from special_neurons import find_linear_layer_pairs, ordered_magnitude_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "linear_layer_pairs = find_linear_layer_pairs(model)\n",
    "LAYER_PLOT = 0\n",
    "\n",
    "plt.plot(ordered_magnitude_output(linear_layer_pairs[LAYER_PLOT])[0].flatten().sort()[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the most negative wire settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "gpt_neox.layers.0.mlp\n",
      "torch.Size([1, 20, 2048])\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "\n",
    "def get_weights_from_transformer_lens(name: str):\n",
    "  #  TODO: this is a hack with gpt_ne_x\n",
    "  match = re.match(r\"gpt_neox\\.layers\\.(\\d+)\\.mlp\", name)\n",
    "  if not match:\n",
    "    raise ValueError(f\"Name {name} does not match the expected pattern\")\n",
    "  print(name)\n",
    "  layer_number = int(match.group(1))\n",
    "  hooked_name = f\"blocks.{layer_number}.mlp.hook_post\"\n",
    "  print(activations[hooked_name].shape)\n",
    "  return hooked_name\n",
    "\n",
    "# Load a model (eg GPT-2 Small)\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_name)\n",
    "# Run the model and get logits and activations\n",
    "logits, activations = model.run_with_cache(\"the umpires, the umpires, the umpires, the umpires\")\n",
    "\n",
    "# For the first MLP\n",
    "LAYER_CHECK = 2\n",
    "trans_lens_name = get_weights_from_transformer_lens(linear_layer_pairs[LAYER_CHECK].prev_layer_name)\n",
    "weights_layer = linear_layer_pairs[LAYER_CHECK].linear_layer.weight\n",
    "activations_prev_layer = activations[trans_lens_name]\n",
    "activations_prev_layer = activations_prev_layer.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 512]), torch.Size([20, 2048]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_layer.shape, activations_prev_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FOR_TOK = 0\n",
    "most_min = float('inf')\n",
    "most_min_index= -1\n",
    "mins = torch.zeros_like(weights_layer[0])\n",
    "for outgoing_wire in range(weights_layer.shape[1]):\n",
    "\tv = min(activations_prev_layer[FOR_TOK] * weights_layer.T[outgoing_wire])\n",
    "\tif v < most_min:\n",
    "\t\tmost_min = v\n",
    "\t\tmost_min_index = outgoing_wire\n",
    "\tmins[outgoing_wire] = v\n",
    "print(\"MINS\", most_min_index, most_min)\n",
    "plt.plot(mins.sort()[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at most negative individual neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaad0385e57408a8597cb5047bd5155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abd8707eae94127bc5c6cabb5890f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd3a0c258f64effa1d80fc1134b6ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Replace MODEL_NAME with the actual model name\n",
    "\n",
    "# Encode the input text\n",
    "input_text = \"The boy with the SFdfdrgfeDSSD\"  # Replace YOUR_INPUT_TEXT with your actual input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# Forward pass to get output logits\n",
    "outputs = model(input_ids, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states  # Hidden states of all layers\n",
    "\n",
    "# Access the specific neuron's value\n",
    "# Replace LAYER_INDEX, BATCH_INDEX, TOKEN_INDEX, NEURON_INDEX with actual indices\n",
    "# neuron_value = hidden_states[LAYER_INDEX][BATCH_INDEX, TOKEN_INDEX, NEURON_INDEX].item()\n",
    "\n",
    "print(hidden_states[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer_lens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6407/344226125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformer_lens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load a model (eg GPT-2 Small)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHookedTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run the model and get logits and activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformer_lens'"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "\n",
    "# Load a model (eg GPT-2 Small)\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_name)\n",
    "# Run the model and get logits and activations\n",
    "logits, activations = model.run_with_cache(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.layers.0.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 227\n",
      "Activations: tensor([0.4309], device='cuda:0') on token 1 with negative value -13.506959915161133\n",
      "Effective change tensor([-5.8199], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 249\n",
      "Activations: tensor([0.3862], device='cuda:0') on token 1 with negative value -74.46073150634766\n",
      "Effective change tensor([-28.7561], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.1.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 47\n",
      "Activations: tensor([1.0227], device='cuda:0') on token 1 with negative value -17.482093811035156\n",
      "Effective change tensor([-17.8796], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 157\n",
      "Activations: tensor([0.5185], device='cuda:0') on token 2 with negative value -21.773366928100586\n",
      "Effective change tensor([-11.2893], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.2.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 111\n",
      "Activations: tensor([0.8778], device='cuda:0') on token 0 with negative value -12.92142391204834\n",
      "Effective change tensor([-11.3418], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 156\n",
      "Activations: tensor([0.3742], device='cuda:0') on token 0 with negative value -24.87030601501465\n",
      "Effective change tensor([-9.3067], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 71\n",
      "Activations: tensor([0.5842], device='cuda:0') on token 2 with negative value -12.548407554626465\n",
      "Effective change tensor([-7.3304], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "Found a match: 157\n",
      "Activations: tensor([0.5477], device='cuda:0') on token 2 with negative value -19.010305404663086\n",
      "Effective change tensor([-10.4110], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.3.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 52\n",
      "Activations: tensor([0.6890], device='cuda:0') on token 1 with negative value -12.224004745483398\n",
      "Effective change tensor([-8.4220], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "gpt_neox.layers.4.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "gpt_neox.layers.5.mlp\n",
      "torch.Size([1, 3, 2048])\n",
      "N Tokens 3\n",
      "Found a match: 279\n",
      "Activations: tensor([2.1954], device='cuda:0') on token 0 with negative value -16.212507247924805\n",
      "Effective change tensor([-35.5936], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_MAX_LOOK = 100\n",
    "\n",
    "# Okie, this is what we want!!\n",
    "\n",
    "\n",
    "def find_logits_on_mlps():\n",
    "    for layer_pair in most_neg:\n",
    "        name = layer_pair.prev_layer_name\n",
    "        match = re.match(r\"gpt_neox\\.layers\\.(\\d+)\\.mlp\", name)\n",
    "        if match:\n",
    "            print(name)\n",
    "            layer_number = int(match.group(1))\n",
    "            hooked_name = f\"blocks.{layer_number}.mlp.hook_post\"\n",
    "            print(activations[hooked_name].shape)\n",
    "            # Find the most positive activations\n",
    "            # TODO: this is weird... it has to be **per token**\n",
    "            most_neg_out = layer_pair.most_negatives.tolist()\n",
    "            n_tokens = activations[hooked_name].shape[1]\n",
    "            print(\"N Tokens\", n_tokens)\n",
    "            for tok_idx in range(n_tokens):\n",
    "                maxed = activations[hooked_name][0, tok_idx].argsort(descending=True)[\n",
    "                    :N_MAX_LOOK]\n",
    "                for m in maxed:\n",
    "                    for j in range(len(most_neg_out)):\n",
    "                        if m == most_neg_out[j]:\n",
    "                            print(f\"Found a match: {m}\")\n",
    "                            # TODO: print token\n",
    "                            print(\n",
    "                                f\"Activations: {activations[hooked_name][:, tok_idx, m]} on token {tok_idx} with negative value {layer_pair.most_negatives_vals[j]}\")\n",
    "                            effective_change = layer_pair.most_negatives_vals[j] * activations[hooked_name][:, tok_idx, m]\n",
    "                            print(\"Effective change\", effective_change.item())\n",
    "                            print()\n",
    "\n",
    "\n",
    "find_logits_on_mlps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
