{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The ONNX File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_neox.layers.0.mlp', 'gpt_neox.layers.0.mlp.dense_h_to_4h')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from special_neurons import get_most_negative_sets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "model_name = 'EleutherAI/pythia-160m'\n",
    "model_name = 'EleutherAI/pythia-70m'\n",
    "# model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model_og = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "most_neg = get_most_negative_sets(model_og)\n",
    "most_neg[0].prev_layer_name, most_neg[0].linear_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 2.0 False\n"
     ]
    }
   ],
   "source": [
    "# Our max_norm is none\n",
    "print(model_og.gpt_neox.embed_in.max_norm, model_og.gpt_neox.embed_in.norm_type, model_og.gpt_neox.embed_in.scale_grad_by_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel_og\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           GPTNeoXForCausalLM\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "GPTNeoXForCausalLM(\n",
      "           (gpt_neox): GPTNeoXModel(\n",
      "           (embed_in): Embedding(50304, 512)\n",
      "           (emb_dr <...> entwise_affine=True)\n",
      "           )\n",
      "           (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      "           )\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "GPTNeoX Model with a `language modeling` head on top for CLM fine-tuning.\n",
      "This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n",
      "it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
      "behavior.\n",
      "\n",
      "Parameters:\n",
      "    config ([`~GPTNeoXConfig`]): Model configuration class with all the parameters of the model.\n",
      "        Initializing with a config file does not load the weights associated with the model, only the\n",
      "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n"
     ]
    }
   ],
   "source": [
    "model_og?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((512,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].input_layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_matrix = model_og.gpt_neox.embed_in.weight\n",
    "embd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[25521,  1533]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_input_after_tokenizer(inp: str):\n",
    "\t\ttokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\t\tinp_out = tokenizer(inp, return_tensors=\"pt\")\n",
    "\t\tprint(inp_out)\n",
    "\t\tinputs = inp_out['input_ids'].squeeze(0)\n",
    "\t\t# print(inputs, inputs.shape)\n",
    "\t\t# TODO: IDK ABOUT WHATS GOING ON W/ TOKEN SIZE VS Vocab Size\n",
    "\t\t# Vocab size is 50204 and inp size is 50304\n",
    "\t\tone_hot = torch.zeros((inputs.shape[0], 50304), dtype=torch.int)\n",
    "\t\tfor i in range(inputs.shape[0]):\n",
    "\t\t\tone_hot[i, inputs[i]] = 1\n",
    "\t\t# one_hot[inputs['input_ids'][0, 0]] = 1\n",
    "\t\treturn one_hot#, inp_out['attention_mask']\n",
    "get_input_after_tokenizer('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[25521,  1533]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 50304]),\n",
       " tensor([[    0, 25521],\n",
       "         [    1,  1533]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps_one_hot = get_input_after_tokenizer(\"hello world\")\n",
    "inps_one_hot.shape, inps_one_hot.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelSel(\n",
       "  (embed_linear): Linear(in_features=50304, out_features=512, bias=False)\n",
       "  (layer_norm): SimplfiedLayerNorm(\n",
       "    (eps): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (ones_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (ones_linear_neg): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (attn): FixedAttentionMask(\n",
       "    (attn): GPTNeoXAttention(\n",
       "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (slice_query): Slicer(\n",
       "      (slicer): Linear(in_features=192, out_features=64, bias=True)\n",
       "    )\n",
       "    (slice_value): Slicer(\n",
       "      (slicer): Linear(in_features=192, out_features=64, bias=True)\n",
       "    )\n",
       "    (slice_key): Slicer(\n",
       "      (slicer): Linear(in_features=192, out_features=64, bias=True)\n",
       "    )\n",
       "    (slice_rotary): Slicer(\n",
       "      (slicer): Linear(in_features=64, out_features=16, bias=True)\n",
       "    )\n",
       "    (slice_non_rotary): Slicer(\n",
       "      (slicer): Linear(in_features=64, out_features=48, bias=True)\n",
       "    )\n",
       "    (slice_rorate_half_1): Slicer(\n",
       "      (slicer): Linear(in_features=16, out_features=8, bias=True)\n",
       "    )\n",
       "    (slice_rorate_half_2): Slicer(\n",
       "      (slicer): Linear(in_features=16, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TOKENS = inps_one_hot.shape[0]\n",
    "attention_mask = torch.ones((1, N_TOKENS), dtype=torch.int)\n",
    "\n",
    "\n",
    "class Slicer(torch.nn.Module):\n",
    "    def __init__(self, inp_dim: int, start_at: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.slicer = torch.nn.Linear(inp_dim, output_dim)\n",
    "        stacked = []\n",
    "        if start_at > 0:\n",
    "            stacked.append(torch.zeros((output_dim, start_at)))\n",
    "        stacked.append(torch.eye(output_dim))\n",
    "        if start_at + output_dim < inp_dim:\n",
    "            stacked.append(torch.zeros((output_dim, inp_dim - output_dim - start_at)))\n",
    "        self.slicer.weight = torch.nn.Parameter(\n",
    "            torch.hstack(stacked)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.slicer(x)\n",
    "\n",
    "class FixedAttentionMask(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attn = model_og.gpt_neox.layers[0].attention\n",
    "        self._init_bias(N_TOKENS)\n",
    "        self.slice_query = Slicer(192, 0, 64)\n",
    "        self.slice_value = Slicer(192, 64, 64)\n",
    "        self.slice_key = Slicer(192, 64 * 2, 64)\n",
    "        self.slice_rotary = Slicer(64, 0, 16)\n",
    "        self.slice_non_rotary = Slicer(64, 16, 64 - 16)\n",
    "        # TODO: I think 16 has to do with having 2 tokens... we need to generalize\n",
    "        self.slice_rorate_half_1 = Slicer(16, 0, 8)\n",
    "        self.slice_rorate_half_2 = Slicer(16, 8, 8)\n",
    "        # ATTENTION BIAS ATTENTION BIAS 2 2 2 2 torch.Size([1, 1, 2048, 2048])\n",
    "        # TODO: PARAMETERIZE BETTER\n",
    "        # self.attn_bias_slice_a = Slicer(2048, 0, 2)\n",
    "        # self.attn_bias_slice_b = Slicer(2048, 0, 2)\n",
    "        # self.attn.bias = self.attn.bias[:, :, :2, :2]\n",
    "    \n",
    "    def _init_bias(self, max_positions, device=None):\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                1, 1, max_positions, max_positions\n",
    "            ),\n",
    "            persistent=False,\n",
    "        )\n",
    "        if device is not None:\n",
    "            self.bias = self.bias.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n",
    "        # compute causal mask from causal mask buffer\n",
    "        num_attention_heads, query_length, attn_head_size = query.size()\n",
    "        key_length = key.size(-2)\n",
    "\n",
    "        # dynamically increase the causal mask with the key length, if needed.\n",
    "        print(\"KEY BIAS\", key_length, key.shape, self.bias.shape[-1])\n",
    "        if key_length > self.bias.shape[-1]:\n",
    "            self._init_bias(N_TOKENS, device=key.device)\n",
    "        print(\"ATTENTION BIAS\", key_length, query_length, key_length, key_length, self.bias.shape)\n",
    "        # causal_mask = self.attn.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "        causal_mask = self.bias\n",
    "        # A cheap way of doing the above: TODO: MAYBE HARDCODE THIS IN!\n",
    "\n",
    "        query = query.reshape(num_attention_heads, query_length, attn_head_size)\n",
    "        key = key.reshape(num_attention_heads, key_length, attn_head_size)\n",
    "        # query = query.view(num_attention_heads, query_length, attn_head_size)\n",
    "        # key = key.view(num_attention_heads, key_length, attn_head_size)\n",
    "        attn_scores = torch.zeros(\n",
    "            num_attention_heads,\n",
    "            query_length,\n",
    "            key_length,\n",
    "            dtype=query.dtype,\n",
    "            device=key.device,\n",
    "        )\n",
    "        attn_scores = torch.baddbmm(\n",
    "            attn_scores,\n",
    "            query,\n",
    "            key.transpose(1, 2),\n",
    "            beta=1.0,\n",
    "            alpha=self.attn.norm_factor,\n",
    "        )\n",
    "        attn_scores = attn_scores.reshape(num_attention_heads, query_length, key_length)\n",
    "\n",
    "        mask_value = torch.finfo(attn_scores.dtype).min\n",
    "        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "        mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n",
    "        attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = attn_weights.to(value.dtype)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        # todo: put back in\n",
    "        # attn_weights = self.attn.attention_dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: IDK IF THIS IS RIGHT for POSITION IDS or ATTENTION MASK\n",
    "        position_ids=torch.arange(N_TOKENS)\n",
    "        # Compute QKV\n",
    "        # Attention heads [seq_len, hidden_size] --> [seq_len, (np * 3 * head_size)]\n",
    "        qkv = self.attn.query_key_value(x)\n",
    "        print(qkv.shape)\n",
    "\n",
    "        # [seq_len, (num_heads * 3 * head_size)] --> [seq_len, num_heads, 3 * head_size]\n",
    "        print(\"QKV SIZE\", qkv.size(), qkv.size()[:-1])\n",
    "\t\t# TODO: I think that this has to be a parameter\n",
    "        QKV_last_size = (N_TOKENS,)\n",
    "        # new_qkv_shape = qkv.size()[:-1] + (self.attn.num_attention_heads, 3 * self.attn.head_size)\n",
    "        new_qkv_shape = QKV_last_size + (self.attn.num_attention_heads, 3 * self.attn.head_size)\n",
    "        qkv = qkv.reshape(*new_qkv_shape)\n",
    "\n",
    "        # [seq_len, num_attention_heads, 3 * head_size] --> 3 [num_attention_heads, seq_len, head_size]\n",
    "        # query = qkv[..., :self.attn.head_size].permute(1, 0, 2)\n",
    "        query = self.slice_query(qkv).permute(1, 0, 2)\n",
    "        key = self.slice_key(qkv).permute(1, 0, 2)\n",
    "        value = self.slice_value(qkv).permute(1, 0, 2)\n",
    "\n",
    "        # Compute rotary embeddings on rotary_ndims\n",
    "        # 16 and 64\n",
    "        #query_rot = query[..., :self.attn.rotary_ndims]\n",
    "        #query_pass = query[..., self.attn.rotary_ndims:]\n",
    "        #key_rot = key[..., :self.attn.rotary_ndims]\n",
    "        #key_pass = key[..., self.attn.rotary_ndims:]\n",
    "        query_rot = self.slice_rotary(query)\n",
    "        query_pass = self.slice_non_rotary(query)\n",
    "        key_rot = self.slice_rotary(key)\n",
    "        key_pass = self.slice_non_rotary(key)\n",
    "\n",
    "        # Compute token offset for rotary embeddings (when decoding)\n",
    "        print(\"SEQ LEN\", key.shape)\n",
    "        seq_len = key.shape[-2]\n",
    "        cos, sin = self.attn.rotary_emb(value, seq_len=seq_len)\n",
    "\n",
    "\n",
    "        def rotate_half(x):\n",
    "            \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "            print(\"ROTATE HALF SLICE DIMS\", x.shape)\n",
    "            # x1 = x[..., : x.shape[-1] // 2]\n",
    "            # x2 = x[..., x.shape[-1] // 2 :]\n",
    "            x1 = self.slice_rorate_half_1(x)\n",
    "            x2 = self.slice_rorate_half_2(x)\n",
    "            return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "        def apply_rotary_embed(q, k, cos, sin, position_ids):\n",
    "            cos = cos[position_ids]\n",
    "            sin = sin[position_ids]\n",
    "            q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "            k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "            return q_embed, k_embed\n",
    "        print(query_rot.shape, key_rot.shape, cos.shape, sin.shape, position_ids.shape)\n",
    "        query, key = apply_rotary_embed(query_rot, key_rot, cos, sin, position_ids)\n",
    "        query = torch.cat((query, query_pass), dim=-1)\n",
    "        key = torch.cat((key, key_pass), dim=-1)\n",
    "\n",
    "        # Cache QKV values\n",
    "        # if has_layer_past:\n",
    "        #     past_key = layer_past[0]\n",
    "        #     past_value = layer_past[1]\n",
    "        #     key = torch.cat((past_key, key), dim=-2)\n",
    "        #     value = torch.cat((past_value, value), dim=-2)\n",
    "        present = None\n",
    "\n",
    "        # Compute attention\n",
    "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, None)\n",
    "\n",
    "        # Reshape outputs\n",
    "        attn_output = self.attn._merge_heads(attn_output, self.attn.num_attention_heads, self.attn.head_size)\n",
    "        attn_output = self.attn.dense(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        # if output_attentions:\n",
    "        #     outputs += (attn_weights,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "        return self.attn(x, attention_mask=attention_mask, position_ids=torch.arange(N_TOKENS).unsqueeze(0))\n",
    "\n",
    "\n",
    "embed_linear = torch.nn.Linear(\n",
    "    embd_matrix.shape[0], embd_matrix.shape[1], bias=False)\n",
    "embed_linear.weight = torch.nn.Parameter(embd_matrix.T)\n",
    "\n",
    "\n",
    "class SimplfiedLayerNorm(torch.nn.Module):\n",
    "    def __init__(self, layernorm: torch.nn.LayerNorm) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = layernorm.weight\n",
    "        self.bias = layernorm.bias\n",
    "        self.eps = torch.nn.Linear(512, 512)\n",
    "        self.eps.weight = torch.nn.Parameter(torch.eye(512))\n",
    "        self.eps.bias = torch.nn.Parameter(torch.ones(512) * 1e-5)\n",
    "        # TODO: not constant\n",
    "        self.ones_linear = torch.nn.Linear(512, 512, bias=False)\n",
    "        self.ones_linear.weight = torch.nn.Parameter(torch.ones((512, 512)))\n",
    "        self.ones_linear_neg = torch.nn.Linear(512, 512, bias=False)\n",
    "        self.ones_linear_neg.weight = torch.nn.Parameter(-1 * torch.ones((512, 512)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: I think that this can be made more efficient\n",
    "        expectation_neg = self.ones_linear_neg(x)\n",
    "        variance = self.ones_linear((((x + expectation_neg) * (x + expectation_neg))))\n",
    "        radical = self.eps(variance)\n",
    "        denom = torch.sqrt(radical)\n",
    "        x = x + expectation_neg\n",
    "        # TODO: DENOM HAS PROVLEMS\n",
    "        return x\n",
    "        x = x / denom\n",
    "        return x\n",
    "        x = x * self.weight\n",
    "        x = x + self.bias\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModelSel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed_linear = embed_linear\n",
    "        self.layer_norm = SimplfiedLayerNorm(model_og.gpt_neox.layers[0].input_layernorm)\n",
    "        self.attn = FixedAttentionMask()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed_linear(x)\n",
    "        x = self.layer_norm(x)\n",
    "        # HRMM Unsqueeze no good\n",
    "        # x = x.reshape((1, *x.shape))\n",
    "        x = self.attn(x)\n",
    "        return x[0]\n",
    "\n",
    "# G\n",
    "# # TODO: add residuals?\n",
    "# model_sel = torch.nn.Sequential(\n",
    "#     # model_og.gpt_neox.embed_in,\n",
    "#     embed_linear,\n",
    "#     # model_og.gpt_neox.emb_dropout, # we have p = 0.0 and thus useless\n",
    "#     model_og.gpt_neox.layers[0].input_layernorm,\n",
    "#     FixedAttentionMask(),\n",
    "#     # TODO: VERIFY THIS JAZZ\n",
    "# )\n",
    "\n",
    "\n",
    "model_sel = ModelSel()\n",
    "model_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1536])\n",
      "QKV SIZE torch.Size([2, 1536]) torch.Size([2])\n",
      "SEQ LEN torch.Size([8, 2, 64])\n",
      "torch.Size([8, 2, 16]) torch.Size([8, 2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([8, 2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([8, 2, 16])\n",
      "KEY BIAS 2 torch.Size([8, 2, 64]) 2\n",
      "ATTENTION BIAS 2 2 2 2 torch.Size([1, 1, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps_one_hot.shape, embed_linear.weight.shape\n",
    "inps_one_hot_formatted = inps_one_hot.float()\n",
    "# embed_linear.forward(inps_one_hot).shape\n",
    "model_sel(inps_one_hot_formatted)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1536])\n",
      "QKV SIZE torch.Size([2, 1536]) torch.Size([2])\n",
      "SEQ LEN torch.Size([8, 2, 64])\n",
      "torch.Size([8, 2, 16]) torch.Size([8, 2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n",
      "ROTATE HALF SLICE DIMS torch.Size([8, 2, 16])\n",
      "ROTATE HALF SLICE DIMS torch.Size([8, 2, 16])\n",
      "KEY BIAS tensor(2) torch.Size([8, 2, 64]) tensor(2)\n",
      "ATTENTION BIAS tensor(2) tensor(2) tensor(2) tensor(2) torch.Size([1, 1, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:557: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/tmp/ipykernel_3840/3217107551.py:63: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if key_length > self.bias.shape[-1]:\n",
      "/tmp/ipykernel_3840/3217107551.py:93: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model_sel, inps_one_hot_formatted,\n",
    "                  'model_sel.onnx', verbose=False, opset_version=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Names: ['onnx::MatMul_0']\n",
      "Output Names: ['134']\n",
      "0 Identity_14\n",
      "1 Identity_15\n",
      "2 Identity_16\n",
      "3 Identity_17\n",
      "4 /embed_linear/MatMul\n",
      "5 /layer_norm/ones_linear_neg/MatMul\n",
      "6 /layer_norm/Add\n",
      "7 /attn/query_key_value/Gemm\n",
      "8 /attn/Constant\n",
      "9 /attn/Reshape\n",
      "10 /attn/slice_query/slicer/MatMul\n",
      "11 /attn/slice_query/slicer/Add\n",
      "12 /attn/Transpose\n",
      "13 /attn/slice_key/slicer/MatMul\n",
      "14 /attn/slice_key/slicer/Add\n",
      "15 /attn/Transpose_1\n",
      "16 /attn/slice_value/slicer/MatMul\n",
      "17 /attn/slice_value/slicer/Add\n",
      "18 /attn/Transpose_2\n",
      "19 /attn/slice_rotary/slicer/MatMul\n",
      "20 /attn/slice_rotary/slicer/Add\n",
      "21 /attn/slice_non_rotary/slicer/MatMul\n",
      "22 /attn/slice_non_rotary/slicer/Add\n",
      "23 /attn/slice_rotary/slicer_1/MatMul\n",
      "24 /attn/slice_rotary/slicer_1/Add\n",
      "25 /attn/slice_non_rotary/slicer_1/MatMul\n",
      "26 /attn/slice_non_rotary/slicer_1/Add\n",
      "27 /attn/Constant_1\n",
      "28 /attn/Mul\n",
      "29 /attn/slice_rorate_half_1/slicer/MatMul\n",
      "30 /attn/slice_rorate_half_1/slicer/Add\n",
      "31 /attn/slice_rorate_half_2/slicer/MatMul\n",
      "32 /attn/slice_rorate_half_2/slicer/Add\n",
      "33 /attn/Neg\n",
      "34 /attn/Concat\n",
      "35 /attn/Constant_2\n",
      "36 /attn/Mul_1\n",
      "37 /attn/Add\n",
      "38 /attn/Constant_3\n",
      "39 /attn/Mul_2\n",
      "40 /attn/slice_rorate_half_1/slicer_1/MatMul\n",
      "41 /attn/slice_rorate_half_1/slicer_1/Add\n",
      "42 /attn/slice_rorate_half_2/slicer_1/MatMul\n",
      "43 /attn/slice_rorate_half_2/slicer_1/Add\n",
      "44 /attn/Neg_1\n",
      "45 /attn/Concat_1\n",
      "46 /attn/Constant_4\n",
      "47 /attn/Mul_3\n",
      "48 /attn/Add_1\n",
      "49 /attn/Concat_2\n",
      "50 /attn/Concat_3\n",
      "51 /attn/Constant_5\n",
      "52 /attn/Constant_6\n",
      "53 /attn/Reshape_1\n",
      "54 /attn/Constant_7\n",
      "55 /attn/Reshape_2\n",
      "56 /attn/Constant_8\n",
      "57 /attn/Transpose_3\n",
      "58 /attn/MatMul\n",
      "59 /attn/Constant_9\n",
      "60 /attn/Mul_4\n",
      "61 /attn/Constant_10\n",
      "62 /attn/Add_2\n",
      "63 /attn/Reshape_3\n",
      "64 Constant_78\n",
      "65 /attn/Where\n",
      "66 /attn/Constant_11\n",
      "67 /attn/Add_3\n",
      "68 /attn/Softmax\n",
      "69 /attn/Cast\n",
      "70 /attn/MatMul_1\n",
      "71 /attn/Transpose_4\n",
      "72 /attn/Constant_12\n",
      "73 /attn/Reshape_4\n",
      "74 /attn/dense/MatMul\n",
      "75 /attn/dense/Add\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"model_sel.onnx\")\n",
    "\n",
    "# Get the names of input and output nodes\n",
    "input_names = [input.name for input in model.graph.input]\n",
    "output_names = [output.name for output in model.graph.output]\n",
    "\n",
    "print(\"Input Names:\", input_names)\n",
    "print(\"Output Names:\", output_names)\n",
    "\n",
    "model_onnx = onnx.load('model_sel.onnx')\n",
    "for i, node in enumerate(model.graph.node):\n",
    "    if node.op_type == 'Gather':\n",
    "        print(f\"'Gather' found in node {i}: {node.name}\")\n",
    "    else:\n",
    "        print(i, node.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Mariboupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTHONPATH=\"$PYTHONPATH:/home/lev/code/research/ai/dictator/Marabou\"\n",
    "# !export PYTHONPATH=\"$PYTHONPATH:/home/lev/code/research/ai/dictator/Marabou/maraboupy\"\n",
    "# !pip install onnx onnxruntime maraboupy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in /home/lev/.local/lib/python3.9/site-packages (1.15.0)\n",
      "Requirement already satisfied: onnxruntime in /home/lev/.local/lib/python3.9/site-packages (1.17.1)\n",
      "Requirement already satisfied: numpy in /home/lev/.local/lib/python3.9/site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/lev/.local/lib/python3.9/site-packages (from onnx) (3.20.3)\n",
      "Requirement already satisfied: coloredlogs in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: packaging in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (23.1)\n",
      "Requirement already satisfied: sympy in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/lev/.local/lib/python3.9/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lev/.local/lib/python3.9/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lev/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "## %\n",
    "# Path to Marabou folder if you did not export it\n",
    "\n",
    "# TODO: this is a hack, fix it\n",
    "# TODO: add Marabou installation details to readme\n",
    "# sys.path.append('/home/lev/code/research/softmax_bound/bounding-softmax/Marabou')\n",
    "sys.path.append('/home/lev/code/research/ai/dictator/Marabou')\n",
    "from maraboupy import Marabou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Attention Head\n"
     ]
    }
   ],
   "source": [
    "options = Marabou.createOptions(verbosity = 1)\n",
    "print(\"Simple Attention Head\")\n",
    "filename = \"model_sel.onnx\"\n",
    "network = Marabou.read_onnx(filename)#, inputNames=inputNames, outputNames=[outputName])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around with basic constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
