{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def find_negative_nancies():\n",
    "\tpass\n",
    "\n",
    "model_name = 'EleutherAI/pythia-70m'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of gpt_neox.layers.0.attention.query_key_value: torch.Size([1536, 512])\n",
      "Weights of gpt_neox.layers.0.attention.dense: torch.Size([512, 512])\n",
      "Weights of gpt_neox.layers.0.mlp.dense_h_to_4h: torch.Size([2048, 512])\n",
      "Weights of gpt_neox.layers.0.mlp.dense_4h_to_h: torch.Size([512, 2048])\n",
      "Weights of gpt_neox.layers.1.attention.query_key_value: torch.Size([1536, 512])\n",
      "Weights of gpt_neox.layers.1.attention.dense: torch.Size([512, 512])\n",
      "Weights of gpt_neox.layers.1.mlp.dense_h_to_4h: torch.Size([2048, 512])\n",
      "Weights of gpt_neox.layers.1.mlp.dense_4h_to_h: torch.Size([512, 2048])\n",
      "Weights of gpt_neox.layers.2.attention.query_key_value: torch.Size([1536, 512])\n",
      "Weights of gpt_neox.layers.2.attention.dense: torch.Size([512, 512])\n",
      "Weights of gpt_neox.layers.2.mlp.dense_h_to_4h: torch.Size([2048, 512])\n",
      "Weights of gpt_neox.layers.2.mlp.dense_4h_to_h: torch.Size([512, 2048])\n",
      "Weights of gpt_neox.layers.3.attention.query_key_value: torch.Size([1536, 512])\n",
      "Weights of gpt_neox.layers.3.attention.dense: torch.Size([512, 512])\n",
      "Weights of gpt_neox.layers.3.mlp.dense_h_to_4h: torch.Size([2048, 512])\n",
      "Weights of gpt_neox.layers.3.mlp.dense_4h_to_h: torch.Size([512, 2048])\n",
      "Weights of gpt_neox.layers.4.attention.query_key_value: torch.Size([1536, 512])\n",
      "Weights of gpt_neox.layers.4.attention.dense: torch.Size([512, 512])\n",
      "Weights of gpt_neox.layers.4.mlp.dense_h_to_4h: torch.Size([2048, 512])\n",
      "Weights of gpt_neox.layers.4.mlp.dense_4h_to_h: torch.Size([512, 2048])\n",
      "Weights of gpt_neox.layers.5.attention.query_key_value: torch.Size([1536, 512])\n",
      "Weights of gpt_neox.layers.5.attention.dense: torch.Size([512, 512])\n",
      "Weights of gpt_neox.layers.5.mlp.dense_h_to_4h: torch.Size([2048, 512])\n",
      "Weights of gpt_neox.layers.5.mlp.dense_4h_to_h: torch.Size([512, 2048])\n",
      "Weights of embed_out: torch.Size([50304, 512])\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your neural network model\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Linear):  # Check if the layer is a linear layer\n",
    "        weights = layer.weight.data  # Get the weights of the layer\n",
    "        print(f\"Weights of {name}: {weights.size()}\")  # Print the size of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of layer before gpt_neox.layers.0.attention.dense: torch.Size([1536, 512])\n",
      "Weights of layer before gpt_neox.layers.0.mlp.dense_4h_to_h: torch.Size([2048, 512])\n",
      "Weights of layer before gpt_neox.layers.1.attention.dense: torch.Size([1536, 512])\n",
      "Weights of layer before gpt_neox.layers.1.mlp.dense_4h_to_h: torch.Size([2048, 512])\n",
      "Weights of layer before gpt_neox.layers.2.attention.dense: torch.Size([1536, 512])\n",
      "Weights of layer before gpt_neox.layers.2.mlp.dense_4h_to_h: torch.Size([2048, 512])\n",
      "Weights of layer before gpt_neox.layers.3.attention.dense: torch.Size([1536, 512])\n",
      "Weights of layer before gpt_neox.layers.3.mlp.dense_4h_to_h: torch.Size([2048, 512])\n",
      "Weights of layer before gpt_neox.layers.4.attention.dense: torch.Size([1536, 512])\n",
      "Weights of layer before gpt_neox.layers.4.mlp.dense_4h_to_h: torch.Size([2048, 512])\n",
      "Weights of layer before gpt_neox.layers.5.attention.dense: torch.Size([1536, 512])\n",
      "Weights of layer before gpt_neox.layers.5.mlp.dense_4h_to_h: torch.Size([2048, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "prev_and_linear_layers = []\n",
    "\n",
    "# Assuming 'model' is your neural network model\n",
    "prev_layer = None  # To store the previous layer\n",
    "prev_name = \"\"\n",
    "for name, layer in model.named_modules():  # Iterate through layers\n",
    "  if isinstance(layer, nn.Linear):  # Check if current layer is a linear layer\n",
    "    if prev_layer is not None:  # Check if there is a previous layer\n",
    "        prev_and_linear_layers.append(((prev_name, prev_layer), (name, layer)))\n",
    "        # Process the previous layer, as it is immediately before a linear layer\n",
    "        if isinstance(prev_layer, nn.Linear):  # Example: check if the prev layer is linear\n",
    "            weights = prev_layer.weight.data  # Get the weights\n",
    "            print(f\"Weights of layer before {name}: {weights.size()}\")\n",
    "        # Add more conditions here if you want to handle other layer types\n",
    "    else:\n",
    "        print(f\"No preceding layer found before the first Linear layer {name}\")\n",
    "  prev_layer = layer  # Update prev_layer for the next iteration\n",
    "  prev_name = name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('gpt_neox.layers.0.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True)),\n",
       "  ('gpt_neox.layers.0.mlp.dense_4h_to_h',\n",
       "   Linear(in_features=2048, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.1.attention.rotary_emb', GPTNeoXRotaryEmbedding()),\n",
       "  ('gpt_neox.layers.1.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True))),\n",
       " (('gpt_neox.layers.1.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True)),\n",
       "  ('gpt_neox.layers.1.attention.dense',\n",
       "   Linear(in_features=512, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.1.mlp',\n",
       "   GPTNeoXMLP(\n",
       "     (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "     (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "     (act): GELUActivation()\n",
       "   )),\n",
       "  ('gpt_neox.layers.1.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True))),\n",
       " (('gpt_neox.layers.1.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True)),\n",
       "  ('gpt_neox.layers.1.mlp.dense_4h_to_h',\n",
       "   Linear(in_features=2048, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.2.attention.rotary_emb', GPTNeoXRotaryEmbedding()),\n",
       "  ('gpt_neox.layers.2.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True))),\n",
       " (('gpt_neox.layers.2.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True)),\n",
       "  ('gpt_neox.layers.2.attention.dense',\n",
       "   Linear(in_features=512, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.2.mlp',\n",
       "   GPTNeoXMLP(\n",
       "     (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "     (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "     (act): GELUActivation()\n",
       "   )),\n",
       "  ('gpt_neox.layers.2.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True))),\n",
       " (('gpt_neox.layers.2.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True)),\n",
       "  ('gpt_neox.layers.2.mlp.dense_4h_to_h',\n",
       "   Linear(in_features=2048, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.3.attention.rotary_emb', GPTNeoXRotaryEmbedding()),\n",
       "  ('gpt_neox.layers.3.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True))),\n",
       " (('gpt_neox.layers.3.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True)),\n",
       "  ('gpt_neox.layers.3.attention.dense',\n",
       "   Linear(in_features=512, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.3.mlp',\n",
       "   GPTNeoXMLP(\n",
       "     (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "     (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "     (act): GELUActivation()\n",
       "   )),\n",
       "  ('gpt_neox.layers.3.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True))),\n",
       " (('gpt_neox.layers.3.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True)),\n",
       "  ('gpt_neox.layers.3.mlp.dense_4h_to_h',\n",
       "   Linear(in_features=2048, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.4.attention.rotary_emb', GPTNeoXRotaryEmbedding()),\n",
       "  ('gpt_neox.layers.4.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True))),\n",
       " (('gpt_neox.layers.4.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True)),\n",
       "  ('gpt_neox.layers.4.attention.dense',\n",
       "   Linear(in_features=512, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.4.mlp',\n",
       "   GPTNeoXMLP(\n",
       "     (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "     (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "     (act): GELUActivation()\n",
       "   )),\n",
       "  ('gpt_neox.layers.4.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True))),\n",
       " (('gpt_neox.layers.4.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True)),\n",
       "  ('gpt_neox.layers.4.mlp.dense_4h_to_h',\n",
       "   Linear(in_features=2048, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.5.attention.rotary_emb', GPTNeoXRotaryEmbedding()),\n",
       "  ('gpt_neox.layers.5.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True))),\n",
       " (('gpt_neox.layers.5.attention.query_key_value',\n",
       "   Linear(in_features=512, out_features=1536, bias=True)),\n",
       "  ('gpt_neox.layers.5.attention.dense',\n",
       "   Linear(in_features=512, out_features=512, bias=True))),\n",
       " (('gpt_neox.layers.5.mlp',\n",
       "   GPTNeoXMLP(\n",
       "     (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "     (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "     (act): GELUActivation()\n",
       "   )),\n",
       "  ('gpt_neox.layers.5.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True))),\n",
       " (('gpt_neox.layers.5.mlp.dense_h_to_4h',\n",
       "   Linear(in_features=512, out_features=2048, bias=True)),\n",
       "  ('gpt_neox.layers.5.mlp.dense_4h_to_h',\n",
       "   Linear(in_features=2048, out_features=512, bias=True))),\n",
       " (('gpt_neox.final_layer_norm',\n",
       "   LayerNorm((512,), eps=1e-05, elementwise_affine=True)),\n",
       "  ('embed_out', Linear(in_features=512, out_features=50304, bias=False)))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simplicity, we will only be looking at the linear layers while we build up the code\n",
    "prev_and_linear_layers = prev_and_linear_layers[3:]\n",
    "prev_and_linear_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "FOR LAYER 0 gpt_neox.layers.0.mlp.dense_h_to_4h gpt_neox.layers.0.mlp.dense_4h_to_h\n",
      "torch.Size([512, 2048])\n",
      "##################################################\n",
      "FOR LAYER 1 gpt_neox.layers.1.attention.rotary_emb gpt_neox.layers.1.attention.query_key_value\n",
      "torch.Size([1536, 512])\n",
      "##################################################\n",
      "FOR LAYER 2 gpt_neox.layers.1.attention.query_key_value gpt_neox.layers.1.attention.dense\n",
      "torch.Size([512, 512])\n",
      "##################################################\n",
      "FOR LAYER 3 gpt_neox.layers.1.mlp gpt_neox.layers.1.mlp.dense_h_to_4h\n",
      "torch.Size([2048, 512])\n",
      "This is a good layer with neuron tensor(23, device='cuda:0')\n",
      "##################################################\n",
      "FOR LAYER 4 gpt_neox.layers.1.mlp.dense_h_to_4h gpt_neox.layers.1.mlp.dense_4h_to_h\n",
      "torch.Size([512, 2048])\n",
      "##################################################\n",
      "FOR LAYER 5 gpt_neox.layers.2.attention.rotary_emb gpt_neox.layers.2.attention.query_key_value\n",
      "torch.Size([1536, 512])\n",
      "##################################################\n",
      "FOR LAYER 6 gpt_neox.layers.2.attention.query_key_value gpt_neox.layers.2.attention.dense\n",
      "torch.Size([512, 512])\n",
      "##################################################\n",
      "FOR LAYER 7 gpt_neox.layers.2.mlp gpt_neox.layers.2.mlp.dense_h_to_4h\n",
      "torch.Size([2048, 512])\n",
      "This is a good layer with neuron tensor(156, device='cuda:0')\n",
      "##################################################\n",
      "FOR LAYER 8 gpt_neox.layers.2.mlp.dense_h_to_4h gpt_neox.layers.2.mlp.dense_4h_to_h\n",
      "torch.Size([512, 2048])\n",
      "##################################################\n",
      "FOR LAYER 9 gpt_neox.layers.3.attention.rotary_emb gpt_neox.layers.3.attention.query_key_value\n",
      "torch.Size([1536, 512])\n",
      "##################################################\n",
      "FOR LAYER 10 gpt_neox.layers.3.attention.query_key_value gpt_neox.layers.3.attention.dense\n",
      "torch.Size([512, 512])\n",
      "##################################################\n",
      "FOR LAYER 11 gpt_neox.layers.3.mlp gpt_neox.layers.3.mlp.dense_h_to_4h\n",
      "torch.Size([2048, 512])\n",
      "##################################################\n",
      "FOR LAYER 12 gpt_neox.layers.3.mlp.dense_h_to_4h gpt_neox.layers.3.mlp.dense_4h_to_h\n",
      "torch.Size([512, 2048])\n",
      "##################################################\n",
      "FOR LAYER 13 gpt_neox.layers.4.attention.rotary_emb gpt_neox.layers.4.attention.query_key_value\n",
      "torch.Size([1536, 512])\n",
      "##################################################\n",
      "FOR LAYER 14 gpt_neox.layers.4.attention.query_key_value gpt_neox.layers.4.attention.dense\n",
      "torch.Size([512, 512])\n",
      "##################################################\n",
      "FOR LAYER 15 gpt_neox.layers.4.mlp gpt_neox.layers.4.mlp.dense_h_to_4h\n",
      "torch.Size([2048, 512])\n",
      "##################################################\n",
      "FOR LAYER 16 gpt_neox.layers.4.mlp.dense_h_to_4h gpt_neox.layers.4.mlp.dense_4h_to_h\n",
      "torch.Size([512, 2048])\n",
      "##################################################\n",
      "FOR LAYER 17 gpt_neox.layers.5.attention.rotary_emb gpt_neox.layers.5.attention.query_key_value\n",
      "torch.Size([1536, 512])\n",
      "##################################################\n",
      "FOR LAYER 18 gpt_neox.layers.5.attention.query_key_value gpt_neox.layers.5.attention.dense\n",
      "torch.Size([512, 512])\n",
      "##################################################\n",
      "FOR LAYER 19 gpt_neox.layers.5.mlp gpt_neox.layers.5.mlp.dense_h_to_4h\n",
      "torch.Size([2048, 512])\n",
      "##################################################\n",
      "FOR LAYER 20 gpt_neox.layers.5.mlp.dense_h_to_4h gpt_neox.layers.5.mlp.dense_4h_to_h\n",
      "torch.Size([512, 2048])\n",
      "##################################################\n",
      "FOR LAYER 21 gpt_neox.final_layer_norm embed_out\n",
      "torch.Size([50304, 512])\n"
     ]
    }
   ],
   "source": [
    "# Now we have to look at the weights of the next boy in the previous layer\n",
    "def find_negative_nancies(previous_layer, linear_layer: nn.Linear):\n",
    "\t# Each columnd corresponds to the prior layer\n",
    "\tprint(linear_layer.weight.shape)\n",
    "\t# print(previous_layer.weight.shape)\n",
    "\tnegativity = torch.sum(linear_layer.weight < 0, axis=0)\n",
    "\t# print(\"Minimums\", min(negativity))\n",
    "\t# print(\"Maximums\", max(negativity))\n",
    "\tsummed_across_rows = torch.sum(linear_layer.weight, axis=0)\n",
    "\t# print(\"Most Negative\", min(summed_across_rows))\n",
    "\t# print(\"Most negative bit\", summed_across_rows.argmin(), negativity.argmax())\n",
    "\t\n",
    "\tmost_neg_in_val = summed_across_rows.argmin()\n",
    "\tmost_neg_in_l1 = negativity.argmax()\n",
    "\t# Some heuristics\n",
    "\tif most_neg_in_val == most_neg_in_l1 and min(negativity) * 1.5 < max(negativity):\n",
    "\t\tprint(\"This is a good layer with neuron\", most_neg_in_val)\n",
    "\t\treturn most_neg_in_val\n",
    "\treturn -1\n",
    "\n",
    "for i in range(len(prev_and_linear_layers)):\n",
    "\tprint(\"#\" * 50)\n",
    "\tprint(\"FOR LAYER\", i, prev_and_linear_layers[i][0][0], prev_and_linear_layers[i][1][0])\n",
    "\tfind_negative_nancies(prev_and_linear_layers[i][0][1], prev_and_linear_layers[i][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
