{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The ONNX File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_neox.layers.0.mlp', 'gpt_neox.layers.0.mlp.dense_h_to_4h')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from special_neurons import get_most_negative_sets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "model_name = 'EleutherAI/pythia-160m'\n",
    "model_name = 'EleutherAI/pythia-70m'\n",
    "# model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model_og = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "most_neg = get_most_negative_sets(model_og)\n",
    "most_neg[0].prev_layer_name, most_neg[0].linear_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 2.0 False\n"
     ]
    }
   ],
   "source": [
    "# Our max_norm is none\n",
    "print(model_og.gpt_neox.embed_in.max_norm, model_og.gpt_neox.embed_in.norm_type, model_og.gpt_neox.embed_in.scale_grad_by_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel_og\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           GPTNeoXForCausalLM\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "GPTNeoXForCausalLM(\n",
      "           (gpt_neox): GPTNeoXModel(\n",
      "           (embed_in): Embedding(50304, 512)\n",
      "           (emb_dr <...> entwise_affine=True)\n",
      "           )\n",
      "           (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      "           )\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "GPTNeoX Model with a `language modeling` head on top for CLM fine-tuning.\n",
      "This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n",
      "it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
      "behavior.\n",
      "\n",
      "Parameters:\n",
      "    config ([`~GPTNeoXConfig`]): Model configuration class with all the parameters of the model.\n",
      "        Initializing with a config file does not load the weights associated with the model, only the\n",
      "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n"
     ]
    }
   ],
   "source": [
    "model_og?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((512,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.gpt_neox.layers[0].input_layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_matrix = model_og.gpt_neox.embed_in.weight\n",
    "embd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[25521,  1533]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_input_after_tokenizer(inp: str):\n",
    "\t\ttokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\t\tinp_out = tokenizer(inp, return_tensors=\"pt\")\n",
    "\t\tprint(inp_out)\n",
    "\t\tinputs = inp_out['input_ids'].squeeze(0)\n",
    "\t\t# print(inputs, inputs.shape)\n",
    "\t\t# TODO: IDK ABOUT WHATS GOING ON W/ TOKEN SIZE VS Vocab Size\n",
    "\t\t# Vocab size is 50204 and inp size is 50304\n",
    "\t\tone_hot = torch.zeros((inputs.shape[0], 50304), dtype=torch.int)\n",
    "\t\tfor i in range(inputs.shape[0]):\n",
    "\t\t\tone_hot[i, inputs[i]] = 1\n",
    "\t\t# one_hot[inputs['input_ids'][0, 0]] = 1\n",
    "\t\treturn one_hot#, inp_out['attention_mask']\n",
    "get_input_after_tokenizer('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[25521,  1533]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 50304]),\n",
       " tensor([[    0, 25521],\n",
       "         [    1,  1533]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps_one_hot = get_input_after_tokenizer(\"hello world\")\n",
    "inps_one_hot.shape, inps_one_hot.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelSel(\n",
       "  (embed_linear): Linear(in_features=50304, out_features=512, bias=False)\n",
       "  (layer_norm): SimplfiedLayerNorm(\n",
       "    (eps): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (ones_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (ones_linear_neg): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (attn): FixedAttentionMask(\n",
       "    (attn): GPTNeoXAttention(\n",
       "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TOKENS = inps_one_hot.shape[0]\n",
    "attention_mask = torch.ones((1, N_TOKENS), dtype=torch.int)\n",
    "\n",
    "\n",
    "class FixedAttentionMask(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attn = model_og.gpt_neox.layers[0].attention\n",
    "    \n",
    "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n",
    "        # compute causal mask from causal mask buffer\n",
    "        num_attention_heads, query_length, attn_head_size = query.size()\n",
    "        key_length = key.size(-2)\n",
    "\n",
    "        # dynamically increase the causal mask with the key length, if needed.\n",
    "        if key_length > self.attn.bias.shape[-1]:\n",
    "            self.attn._init_bias(key_length, device=key.device)\n",
    "        causal_mask = self.attn.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "\n",
    "        query = query.reshape(num_attention_heads, query_length, attn_head_size)\n",
    "        key = key.reshape(num_attention_heads, key_length, attn_head_size)\n",
    "        # query = query.view(num_attention_heads, query_length, attn_head_size)\n",
    "        # key = key.view(num_attention_heads, key_length, attn_head_size)\n",
    "        attn_scores = torch.zeros(\n",
    "            num_attention_heads,\n",
    "            query_length,\n",
    "            key_length,\n",
    "            dtype=query.dtype,\n",
    "            device=key.device,\n",
    "        )\n",
    "        attn_scores = torch.baddbmm(\n",
    "            attn_scores,\n",
    "            query,\n",
    "            key.transpose(1, 2),\n",
    "            beta=1.0,\n",
    "            alpha=self.attn.norm_factor,\n",
    "        )\n",
    "        attn_scores = attn_scores.reshape(num_attention_heads, query_length, key_length)\n",
    "\n",
    "        mask_value = torch.finfo(attn_scores.dtype).min\n",
    "        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "        mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n",
    "        attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = attn_weights.to(value.dtype)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "\t\t# todo: put back in\n",
    "        # attn_weights = self.attn.attention_dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: IDK IF THIS IS RIGHT for POSITION IDS or ATTENTION MASK\n",
    "        position_ids=torch.arange(N_TOKENS)\n",
    "        # Compute QKV\n",
    "        # Attention heads [seq_len, hidden_size] --> [seq_len, (np * 3 * head_size)]\n",
    "        qkv = self.attn.query_key_value(x)\n",
    "        print(qkv.shape)\n",
    "\n",
    "        # [seq_len, (num_heads * 3 * head_size)] --> [seq_len, num_heads, 3 * head_size]\n",
    "        new_qkv_shape = qkv.size()[:-1] + (self.attn.num_attention_heads, 3 * self.attn.head_size)\n",
    "        qkv = qkv.reshape(*new_qkv_shape)\n",
    "\n",
    "        # [seq_len, num_attention_heads, 3 * head_size] --> 3 [num_attention_heads, seq_len, head_size]\n",
    "        query = qkv[..., :self.attn.head_size].permute(1, 0, 2)\n",
    "        key = qkv[..., self.attn.head_size:2 * self.attn.head_size].permute(1, 0, 2)\n",
    "        value = qkv[..., 2 * self.attn.head_size:].permute(1, 0, 2)\n",
    "\n",
    "        # Compute rotary embeddings on rotary_ndims\n",
    "        query_rot = query[..., :self.attn.rotary_ndims]\n",
    "        query_pass = query[..., self.attn.rotary_ndims:]\n",
    "        key_rot = key[..., :self.attn.rotary_ndims]\n",
    "        key_pass = key[..., self.attn.rotary_ndims:]\n",
    "\n",
    "        # Compute token offset for rotary embeddings (when decoding)\n",
    "        seq_len = key.shape[-2]\n",
    "        cos, sin = self.attn.rotary_emb(value, seq_len=seq_len)\n",
    "\n",
    "\n",
    "        def rotate_half(x):\n",
    "            \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "            x1 = x[..., : x.shape[-1] // 2]\n",
    "            x2 = x[..., x.shape[-1] // 2 :]\n",
    "            return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "        def apply_rotary_embed(q, k, cos, sin, position_ids):\n",
    "            cos = cos[position_ids]\n",
    "            sin = sin[position_ids]\n",
    "            q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "            k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "            return q_embed, k_embed\n",
    "        print(query_rot.shape, key_rot.shape, cos.shape, sin.shape, position_ids.shape)\n",
    "        query, key = apply_rotary_embed(query_rot, key_rot, cos, sin, position_ids)\n",
    "        query = torch.cat((query, query_pass), dim=-1)\n",
    "        key = torch.cat((key, key_pass), dim=-1)\n",
    "\n",
    "        # Cache QKV values\n",
    "        # if has_layer_past:\n",
    "        #     past_key = layer_past[0]\n",
    "        #     past_value = layer_past[1]\n",
    "        #     key = torch.cat((past_key, key), dim=-2)\n",
    "        #     value = torch.cat((past_value, value), dim=-2)\n",
    "        present = None\n",
    "\n",
    "        # Compute attention\n",
    "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, None)\n",
    "\n",
    "        # Reshape outputs\n",
    "        attn_output = self.attn._merge_heads(attn_output, self.attn.num_attention_heads, self.attn.head_size)\n",
    "        attn_output = self.attn.dense(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        # if output_attentions:\n",
    "        #     outputs += (attn_weights,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "        return self.attn(x, attention_mask=attention_mask, position_ids=torch.arange(N_TOKENS).unsqueeze(0))\n",
    "\n",
    "\n",
    "embed_linear = torch.nn.Linear(\n",
    "    embd_matrix.shape[0], embd_matrix.shape[1], bias=False)\n",
    "embed_linear.weight = torch.nn.Parameter(embd_matrix.T)\n",
    "\n",
    "\n",
    "class SimplfiedLayerNorm(torch.nn.Module):\n",
    "    def __init__(self, layernorm: torch.nn.LayerNorm) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = layernorm.weight\n",
    "        self.bias = layernorm.bias\n",
    "        self.eps = torch.nn.Linear(512, 512)\n",
    "        self.eps.weight = torch.nn.Parameter(torch.eye(512))\n",
    "        self.eps.bias = torch.nn.Parameter(torch.ones(512) * 1e-5)\n",
    "        # TODO: not constant\n",
    "        self.ones_linear = torch.nn.Linear(512, 512, bias=False)\n",
    "        self.ones_linear.weight = torch.nn.Parameter(torch.ones((512, 512)))\n",
    "        self.ones_linear_neg = torch.nn.Linear(512, 512, bias=False)\n",
    "        self.ones_linear_neg.weight = torch.nn.Parameter(-1 * torch.ones((512, 512)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: I think that this can be made more efficient\n",
    "        expectation_neg = self.ones_linear_neg(x)\n",
    "        variance = self.ones_linear((((x + expectation_neg) * (x + expectation_neg))))\n",
    "        radical = self.eps(variance)\n",
    "        denom = torch.sqrt(radical)\n",
    "        x = x + expectation_neg\n",
    "        # TODO: DENOM HAS PROVLEMS\n",
    "        return x\n",
    "        x = x / denom\n",
    "        return x\n",
    "        x = x * self.weight\n",
    "        x = x + self.bias\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModelSel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed_linear = embed_linear\n",
    "        self.layer_norm = SimplfiedLayerNorm(model_og.gpt_neox.layers[0].input_layernorm)\n",
    "        self.attn = FixedAttentionMask()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed_linear(x)\n",
    "        x = self.layer_norm(x)\n",
    "        # HRMM Unsqueeze no good\n",
    "        # x = x.reshape((1, *x.shape))\n",
    "        x = self.attn(x)\n",
    "        return x[0]\n",
    "\n",
    "# G\n",
    "# # TODO: add residuals?\n",
    "# model_sel = torch.nn.Sequential(\n",
    "#     # model_og.gpt_neox.embed_in,\n",
    "#     embed_linear,\n",
    "#     # model_og.gpt_neox.emb_dropout, # we have p = 0.0 and thus useless\n",
    "#     model_og.gpt_neox.layers[0].input_layernorm,\n",
    "#     FixedAttentionMask(),\n",
    "#     # TODO: VERIFY THIS JAZZ\n",
    "# )\n",
    "\n",
    "\n",
    "model_sel = ModelSel()\n",
    "model_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1536])\n",
      "torch.Size([8, 2, 16]) torch.Size([8, 2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3840/323697417.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minps_one_hot_formatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minps_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# embed_linear.forward(inps_one_hot).shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_sel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minps_one_hot_formatted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3840/3980719353.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# HRMM Unsqueeze no good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# x = x.reshape((1, *x.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3840/3980719353.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Reshape outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36m_merge_heads\u001b[0;34m(cls, tensor, num_attention_heads, attn_head_size)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# tensor [bs, num_attention_heads, seq_len, attn_head_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;31m# -> [bs, seq_len, num_attention_heads, attn_head_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_attention_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattn_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "inps_one_hot.shape, embed_linear.weight.shape\n",
    "inps_one_hot_formatted = inps_one_hot.float()\n",
    "# embed_linear.forward(inps_one_hot).shape\n",
    "model_sel(inps_one_hot_formatted)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1536])\n",
      "torch.Size([8, 2, 16]) torch.Size([8, 2, 16]) torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:557: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/tmp/ipykernel_3840/2164041362.py:18: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if key_length > self.attn.bias.shape[-1]:\n",
      "/tmp/ipykernel_3840/2164041362.py:45: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model_sel, inps_one_hot_formatted,\n",
    "                  'model_sel.onnx', verbose=False, opset_version=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Names: ['onnx::MatMul_0']\n",
      "Output Names: ['157']\n",
      "0 /embed_linear/MatMul\n",
      "1 /layer_norm/ones_linear_neg/MatMul\n",
      "2 /layer_norm/Add\n",
      "3 /attn/query_key_value/Gemm\n",
      "4 /attn/Constant\n",
      "5 /attn/Reshape\n",
      "6 /attn/Constant_1\n",
      "7 /attn/Constant_2\n",
      "8 /attn/Constant_3\n",
      "9 /attn/Constant_4\n",
      "10 /attn/Slice\n",
      "11 /attn/Transpose\n",
      "12 /attn/Constant_5\n",
      "13 /attn/Constant_6\n",
      "14 /attn/Constant_7\n",
      "15 /attn/Constant_8\n",
      "16 /attn/Slice_1\n",
      "17 /attn/Transpose_1\n",
      "18 /attn/Constant_9\n",
      "19 /attn/Constant_10\n",
      "20 /attn/Constant_11\n",
      "21 /attn/Constant_12\n",
      "22 /attn/Slice_2\n",
      "23 /attn/Transpose_2\n",
      "24 /attn/Constant_13\n",
      "25 /attn/Constant_14\n",
      "26 /attn/Constant_15\n",
      "27 /attn/Constant_16\n",
      "28 /attn/Slice_3\n",
      "29 /attn/Constant_17\n",
      "30 /attn/Constant_18\n",
      "31 /attn/Constant_19\n",
      "32 /attn/Constant_20\n",
      "33 /attn/Slice_4\n",
      "34 /attn/Constant_21\n",
      "35 /attn/Constant_22\n",
      "36 /attn/Constant_23\n",
      "37 /attn/Constant_24\n",
      "38 /attn/Slice_5\n",
      "39 /attn/Constant_25\n",
      "40 /attn/Constant_26\n",
      "41 /attn/Constant_27\n",
      "42 /attn/Constant_28\n",
      "43 /attn/Slice_6\n",
      "44 /attn/Constant_29\n",
      "45 /attn/Mul\n",
      "46 /attn/Constant_30\n",
      "47 /attn/Constant_31\n",
      "48 /attn/Constant_32\n",
      "49 /attn/Constant_33\n",
      "50 /attn/Slice_7\n",
      "51 /attn/Constant_34\n",
      "52 /attn/Constant_35\n",
      "53 /attn/Constant_36\n",
      "54 /attn/Constant_37\n",
      "55 /attn/Slice_8\n",
      "56 /attn/Neg\n",
      "57 /attn/Concat\n",
      "58 /attn/Constant_38\n",
      "59 /attn/Mul_1\n",
      "60 /attn/Add\n",
      "61 /attn/Constant_39\n",
      "62 /attn/Mul_2\n",
      "63 /attn/Constant_40\n",
      "64 /attn/Constant_41\n",
      "65 /attn/Constant_42\n",
      "66 /attn/Constant_43\n",
      "67 /attn/Slice_9\n",
      "68 /attn/Constant_44\n",
      "69 /attn/Constant_45\n",
      "70 /attn/Constant_46\n",
      "71 /attn/Constant_47\n",
      "72 /attn/Slice_10\n",
      "73 /attn/Neg_1\n",
      "74 /attn/Concat_1\n",
      "75 /attn/Constant_48\n",
      "76 /attn/Mul_3\n",
      "77 /attn/Add_1\n",
      "78 /attn/Concat_2\n",
      "79 /attn/Concat_3\n",
      "80 /attn/Constant_49\n",
      "81 /attn/Constant_50\n",
      "82 /attn/Reshape_1\n",
      "83 /attn/Constant_51\n",
      "84 /attn/Reshape_2\n",
      "85 /attn/Constant_52\n",
      "86 /attn/Transpose_3\n",
      "87 /attn/MatMul\n",
      "88 /attn/Constant_53\n",
      "89 /attn/Mul_4\n",
      "90 /attn/Constant_54\n",
      "91 /attn/Add_2\n",
      "92 /attn/Reshape_3\n",
      "93 Constant_151\n",
      "94 /attn/Where\n",
      "95 /attn/Constant_55\n",
      "96 /attn/Add_3\n",
      "97 /attn/Softmax\n",
      "98 /attn/Cast\n",
      "99 /attn/MatMul_1\n",
      "100 /attn/Transpose_4\n",
      "101 /attn/Constant_56\n",
      "102 /attn/Reshape_4\n",
      "103 /attn/dense/MatMul\n",
      "104 /attn/dense/Add\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"model_sel.onnx\")\n",
    "\n",
    "# Get the names of input and output nodes\n",
    "input_names = [input.name for input in model.graph.input]\n",
    "output_names = [output.name for output in model.graph.output]\n",
    "\n",
    "print(\"Input Names:\", input_names)\n",
    "print(\"Output Names:\", output_names)\n",
    "\n",
    "model_onnx = onnx.load('model_sel.onnx')\n",
    "for i, node in enumerate(model.graph.node):\n",
    "    if node.op_type == 'Gather':\n",
    "        print(f\"'Gather' found in node {i}: {node.name}\")\n",
    "    else:\n",
    "        print(i, node.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Mariboupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTHONPATH=\"$PYTHONPATH:/home/lev/code/research/ai/dictator/Marabou\"\n",
    "# !export PYTHONPATH=\"$PYTHONPATH:/home/lev/code/research/ai/dictator/Marabou/maraboupy\"\n",
    "# !pip install onnx onnxruntime maraboupy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in /home/lev/.local/lib/python3.9/site-packages (1.15.0)\n",
      "Requirement already satisfied: onnxruntime in /home/lev/.local/lib/python3.9/site-packages (1.17.1)\n",
      "Requirement already satisfied: numpy in /home/lev/.local/lib/python3.9/site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/lev/.local/lib/python3.9/site-packages (from onnx) (3.20.3)\n",
      "Requirement already satisfied: coloredlogs in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: packaging in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (23.1)\n",
      "Requirement already satisfied: sympy in /home/lev/.local/lib/python3.9/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/lev/.local/lib/python3.9/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lev/.local/lib/python3.9/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lev/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "## %\n",
    "# Path to Marabou folder if you did not export it\n",
    "\n",
    "# TODO: this is a hack, fix it\n",
    "# TODO: add Marabou installation details to readme\n",
    "# sys.path.append('/home/lev/code/research/softmax_bound/bounding-softmax/Marabou')\n",
    "sys.path.append('/home/lev/code/research/ai/dictator/Marabou')\n",
    "from maraboupy import Marabou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Attention Head\n"
     ]
    }
   ],
   "source": [
    "options = Marabou.createOptions(verbosity = 1)\n",
    "print(\"Simple Attention Head\")\n",
    "filename = \"model_sel.onnx\"\n",
    "network = Marabou.read_onnx(filename)#, inputNames=inputNames, outputNames=[outputName])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around with basic constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
